{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training of 1-layer NN with basic features\n",
    "In this notebook, a vanilla 1-layer neural network is trained on all the basic-engineered features.\n",
    "The models are stored in our Google Drive as follows:\n",
    "\n",
    "└── Daryna-Diffusion-Lecture/\n",
    "└── data/\n",
    "└── subset/\n",
    "└── models/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── one_layer_nn\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── concat/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── concat_cos/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── hadamard/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── hadamard_cos\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── mean/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── mean_cos/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── sum/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── sum_cos/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports and Constants\n",
    "- Change the path in \"cw_dir\" to your local Google Drive sync location\n",
    "- Specify the dataset version in the CONSTANTS part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-16T17:54:34.443314564Z",
     "start_time": "2023-05-16T17:54:34.401954072Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from custom_dataset import CustomDataset, DataLoader\n",
    "from network import NeuralNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CONSTANTS\n",
    "dataset_version = \"v2\"\n",
    "cw_dir = \"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive/\"\n",
    "data_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english_{dataset_version}\"\n",
    "features_dir = f\"{data_dir}/features\"\n",
    "models_dir = f\"{cw_dir}/models\"\n",
    "train_labels_path = f\"{data_dir}/labels/train_labels_{dataset_version}.pickle\"\n",
    "\n",
    "# CONSTANT DICT KEYS\n",
    "TRAIN = \"train\"\n",
    "DEV = \"dev\"\n",
    "TEST = \"test\"\n",
    "splits = [TRAIN, DEV, TEST]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load Training Labels\n",
    "The training labels are loaded from the pickle file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2356,) [1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load train labels from pickle file as np.array\n",
    "train_labels = np.load(f\"{train_labels_path}\", allow_pickle=True)\n",
    "print(train_labels.shape, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T17:54:36.370200944Z",
     "start_time": "2023-05-16T17:54:36.353108970Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Load Engineered Features\n",
    "All engineered feature matrices are loaded from their respective pickle files.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2356, 1536)\n"
     ]
    }
   ],
   "source": [
    "# Initialize dict that maps a feature engineering method to its feature matrix\n",
    "method_to_feat_matrix = {\"concat\": None, \"concat_cos\": None,\n",
    "                         \"sum\": None, \"sum_cos\": None,\n",
    "                         \"mean\": None, \"mean_cos\": None,\n",
    "                         \"hadamard\": None, \"hadamard_cos\": None}\n",
    "\n",
    "# Load feature matrix for every method\n",
    "for method in method_to_feat_matrix.keys():\n",
    "    method_to_feat_matrix[method] = np.load(f\"{features_dir}/{method}/{method}_train_{dataset_version}.pickle\", allow_pickle=True)\n",
    "\n",
    "# Spot check\n",
    "print(method_to_feat_matrix[\"concat\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T17:54:38.017143691Z",
     "start_time": "2023-05-16T17:54:37.987393128Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Set up the Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Dataloader for one exemplary feature matrix \"concat\"\n",
    "batch_size = 1\n",
    "features = method_to_feat_matrix[\"concat\"]\n",
    "train_data = CustomDataset(features, train_labels)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T17:54:39.676669188Z",
     "start_time": "2023-05-16T17:54:39.660076351Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Set up the Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Set up the Network\n",
    "input_dim = len(train_data.__getitem__(0)[0])\n",
    "hidden_dim = 1\n",
    "output_dim = 1\n",
    "network = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Training params\n",
    "lr = 0.1\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "num_epochs = 1\n",
    "losses = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T17:54:41.169932573Z",
     "start_time": "2023-05-16T17:54:41.152220742Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for features, labels in train_dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = network(features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(out, labels.unsqueeze(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T17:54:43.413216744Z",
     "start_time": "2023-05-16T17:54:42.488386733Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cw1_kernel",
   "language": "python",
   "display_name": "cw1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
