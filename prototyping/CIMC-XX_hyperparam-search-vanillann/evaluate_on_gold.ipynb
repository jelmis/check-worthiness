{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and Constants\n",
    "- Select user before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "############## AUTORELOAD MAGIC ###################\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "###################################################\n",
    "\n",
    "############## FUNDAMENTAL MODULES ################\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " ##################################################\n",
    "\n",
    "############## TASK-SPECIFIC MODULES ##############\n",
    "sys.path.append(os.path.join(os.getcwd(),\"src\"))\n",
    "from vanilla_dataset import VanillaDataset\n",
    "from vanilla_nn import VanillaNN\n",
    "from trainer import Trainer\n",
    "from eval import evaluate_model, write_eval_to_file, pretty_print_metrics\n",
    "###################################################\n",
    "\n",
    "\n",
    "####################### CONSTANTS ########################\n",
    "SPLITS = [\"train\", \"dev\", \"test\", \"gold\"]\n",
    "TRAIN, DEV, TEST, TXT, IMG = \"train\", \"dev\", \"test\", \"txt\", \"img\"\n",
    "FE_METHODS = [\"txt_embeddings\", \"img_embeddings\", \"concat\", \"sum\", \"mean\", \"hadamard\"]\n",
    "GOLD = \"gold\"\n",
    "#FE_METHODS += [\"concat_cos\", \"sum_cos\", \"mean_cos\", \"hadamard_cos\"]\n",
    "##########################################################\n",
    "\n",
    "############## DATA SCIENCE & ML MODULES #################\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "##########################################################\n",
    "\n",
    "####################### SELECT ###########################\n",
    "users = [\"patriziopalmisano\", \"onurdenizguler\", \"jockl\"]\n",
    "user = users[2] # SELECT USER\n",
    "version = \"v2\" # SELECT DATASET VERSION\n",
    "dataset_version = version\n",
    "##########################################################\n",
    "\n",
    "if user in users[:2]:\n",
    "    data_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/data/CT23_1A_checkworthy_multimodal_english_{version}\"\n",
    "    cw_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive\"\n",
    "\n",
    "else:\n",
    "    data_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_{dataset_version}\"\n",
    "    cw_dir = \"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive\"\n",
    "\n",
    "features_dir = f\"{data_dir}/features\"\n",
    "labels_dir = f\"{data_dir}/labels\"\n",
    "models_dir = f\"{cw_dir}/models/vanillann_hyperparam_search\"\n",
    "evals_dir = f\"{models_dir}/gold_evals\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_labels = {split: \n",
    "                        np.load(f\"{labels_dir}/{split}_labels_{dataset_version}.pickle\", allow_pickle=True) if split != \"gold\" else \n",
    "                        np.load(f\"{labels_dir}/{split}_labels.pickle\", allow_pickle=True)\n",
    "                for split in SPLITS}\n",
    "\n",
    "method_to_split_to_data = {fe_method: {\n",
    "                                split: \n",
    "                                        np.load(f\"{features_dir}/{fe_method}/{fe_method}_{split}_{dataset_version}.pickle\", allow_pickle=True) if split != \"gold\" else \n",
    "                                        np.load(f\"{features_dir}/{fe_method}/{fe_method}_{split}.pickle\", allow_pickle=True)\n",
    "                                for split in SPLITS} \n",
    "                        for fe_method in FE_METHODS}\n",
    "\n",
    "method_to_split_to_dataset = {fe_method: {\n",
    "                                split:\n",
    "                                        VanillaDataset(method_to_split_to_data[fe_method][split], split_to_labels[split]) \n",
    "                                for split in SPLITS} \n",
    "                        for fe_method in FE_METHODS}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluate the best models\n",
    "\n",
    "In this section, the best models for different features methods are evaluated on the gold test set. \n",
    "\n",
    "- Every model is evaluated on different prediction thresholds\n",
    "- Evaluation results are written to a txt file along with model properties\n",
    "- Models come from the runs under \"prototyping/CIMC-XX_hyperparam-search-vanillann/runs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for every evaluation\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "shuffle = True\n",
    "output_dim = [1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Txt Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature method, model name, batch size, hidden dims\n",
    "feature_method = \"txt_embeddings\"\n",
    "model_name = \"13-06-2023_01-03_txt_embeddings_32x16_lr_1e-05_batch-size_16_shuffled_f1_0.76\"\n",
    "batch_size = 16\n",
    "hidden_dims = [32, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to store eval data\n",
    "eval_file = f\"{evals_dir}/{feature_method}/{model_name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 736\n",
      "Shape of features in batch: torch.Size([16, 768])\n",
      "Shape of labels in batch: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset and dataloader\n",
    "dataset = method_to_split_to_dataset[feature_method][GOLD]\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "print(f\"Number of test examples: {len(dataset)}\")\n",
    "print(f\"Shape of features in batch: {next(iter(dataloader))[0].shape}\")\n",
    "print(f\"Shape of labels in batch: {next(iter(dataloader))[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaNN(\n",
       "  (sequence): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model_dir = f\"{models_dir}/{feature_method}/{model_name}.pt\"\n",
    "input_dim = [len(dataset[0][0])]\n",
    "init_params = input_dim + hidden_dims + output_dim\n",
    "model = VanillaNN(init_params)\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.1\n",
      "0.0: {'precision': 0.949238578680203, 'recall': 0.4074074074074074, 'f1-score': 0.5701219512195123, 'support': 459}\n",
      "1.0: {'precision': 0.49536178107606677, 'recall': 0.9638989169675091, 'f1-score': 0.6544117647058824, 'support': 277}\n",
      "accuracy: 0.6168478260869565\n",
      "\n",
      "Threshold=0.2\n",
      "0.0: {'precision': 0.8654970760233918, 'recall': 0.644880174291939, 'f1-score': 0.7390761548064918, 'support': 459}\n",
      "1.0: {'precision': 0.5862944162436549, 'recall': 0.8339350180505415, 'f1-score': 0.6885245901639345, 'support': 277}\n",
      "accuracy: 0.7160326086956522\n",
      "\n",
      "Threshold=0.3\n",
      "0.0: {'precision': 0.828125, 'recall': 0.8082788671023965, 'f1-score': 0.8180815876515986, 'support': 459}\n",
      "1.0: {'precision': 0.6944444444444444, 'recall': 0.7220216606498195, 'f1-score': 0.7079646017699115, 'support': 277}\n",
      "accuracy: 0.7758152173913043\n",
      "\n",
      "Threshold=0.4\n",
      "0.0: {'precision': 0.796, 'recall': 0.8671023965141612, 'f1-score': 0.8300312825860271, 'support': 459}\n",
      "1.0: {'precision': 0.7415254237288136, 'recall': 0.631768953068592, 'f1-score': 0.682261208576998, 'support': 277}\n",
      "accuracy: 0.7785326086956522\n",
      "\n",
      "Threshold=0.5\n",
      "0.0: {'precision': 0.7586206896551724, 'recall': 0.9106753812636166, 'f1-score': 0.8277227722772277, 'support': 459}\n",
      "1.0: {'precision': 0.7783783783783784, 'recall': 0.51985559566787, 'f1-score': 0.6233766233766234, 'support': 277}\n",
      "accuracy: 0.7635869565217391\n",
      "\n",
      "Threshold=0.6\n",
      "0.0: {'precision': 0.735593220338983, 'recall': 0.9455337690631809, 'f1-score': 0.8274547187797902, 'support': 459}\n",
      "1.0: {'precision': 0.8287671232876712, 'recall': 0.4368231046931408, 'f1-score': 0.5721040189125296, 'support': 277}\n",
      "accuracy: 0.7540760869565217\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on all thresholds\n",
    "metrics_string = \"\"\n",
    "for threshold in thresholds:\n",
    "    scores_dict = evaluate_model(model=model, dataloader=dataloader, confidence=threshold)\n",
    "    metrics_string += pretty_print_metrics(scores_dict, threshold) + \"\\n\"\n",
    "write_eval_to_file(file_path=eval_file, report_string=metrics_string)\n",
    "print(metrics_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Concat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature method, model name, batch size, hidden dims\n",
    "feature_method = \"concat\"\n",
    "model_name = \"13-06-2023_03-03_concat_128x64x32_lr_0.0001_batch-size_64_shuffled_f1_0.75\"\n",
    "batch_size = 64\n",
    "hidden_dims = [128, 64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to store eval data\n",
    "eval_file = f\"{evals_dir}/{feature_method}/{model_name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 736\n",
      "Shape of features in batch: torch.Size([64, 1536])\n",
      "Shape of labels in batch: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset and dataloader\n",
    "dataset = method_to_split_to_dataset[feature_method][GOLD]\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "print(f\"Number of test examples: {len(dataset)}\")\n",
    "print(f\"Shape of features in batch: {next(iter(dataloader))[0].shape}\")\n",
    "print(f\"Shape of labels in batch: {next(iter(dataloader))[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaNN(\n",
       "  (sequence): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model_dir = f\"{models_dir}/{feature_method}/{model_name}.pt\"\n",
    "input_dim = [len(dataset[0][0])]\n",
    "init_params = input_dim + hidden_dims + output_dim\n",
    "model = VanillaNN(init_params)\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.1\n",
      "0.0: {'precision': 0.9023668639053254, 'recall': 0.664488017429194, 'f1-score': 0.7653701380175658, 'support': 459}\n",
      "1.0: {'precision': 0.6130653266331658, 'recall': 0.8808664259927798, 'f1-score': 0.722962962962963, 'support': 277}\n",
      "accuracy: 0.7459239130434783\n",
      "\n",
      "Threshold=0.2\n",
      "0.0: {'precision': 0.8640776699029126, 'recall': 0.775599128540305, 'f1-score': 0.817451205510907, 'support': 459}\n",
      "1.0: {'precision': 0.6820987654320988, 'recall': 0.7978339350180506, 'f1-score': 0.7354409317803662, 'support': 277}\n",
      "accuracy: 0.7839673913043478\n",
      "\n",
      "Threshold=0.3\n",
      "0.0: {'precision': 0.835920177383592, 'recall': 0.8213507625272332, 'f1-score': 0.8285714285714286, 'support': 459}\n",
      "1.0: {'precision': 0.712280701754386, 'recall': 0.7328519855595668, 'f1-score': 0.7224199288256228, 'support': 277}\n",
      "accuracy: 0.7880434782608695\n",
      "\n",
      "Threshold=0.4\n",
      "0.0: {'precision': 0.813141683778234, 'recall': 0.8627450980392157, 'f1-score': 0.8372093023255816, 'support': 459}\n",
      "1.0: {'precision': 0.7469879518072289, 'recall': 0.6714801444043321, 'f1-score': 0.7072243346007605, 'support': 277}\n",
      "accuracy: 0.7907608695652174\n",
      "\n",
      "Threshold=0.5\n",
      "0.0: {'precision': 0.7911025145067698, 'recall': 0.8910675381263616, 'f1-score': 0.8381147540983607, 'support': 459}\n",
      "1.0: {'precision': 0.771689497716895, 'recall': 0.6101083032490975, 'f1-score': 0.6814516129032259, 'support': 277}\n",
      "accuracy: 0.7853260869565217\n",
      "\n",
      "Threshold=0.6\n",
      "0.0: {'precision': 0.7726432532347505, 'recall': 0.9106753812636166, 'f1-score': 0.836, 'support': 459}\n",
      "1.0: {'precision': 0.7897435897435897, 'recall': 0.555956678700361, 'f1-score': 0.652542372881356, 'support': 277}\n",
      "accuracy: 0.7771739130434783\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on all thresholds\n",
    "metrics_string = \"\"\n",
    "for threshold in thresholds:\n",
    "    scores_dict = evaluate_model(model=model, dataloader=dataloader, confidence=threshold)\n",
    "    metrics_string += pretty_print_metrics(scores_dict, threshold) + \"\\n\"\n",
    "write_eval_to_file(file_path=eval_file, report_string=metrics_string)\n",
    "print(metrics_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Mean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature method, model name, batch size, hidden dims\n",
    "feature_method = \"mean\"\n",
    "model_name = \"13-06-2023_05-33_mean_128x64x32x16_lr_0.001_batch-size_8_shuffled_f1_0.71\"\n",
    "batch_size = 8\n",
    "hidden_dims = [128, 64, 32, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to store eval data\n",
    "eval_file = f\"{evals_dir}/{feature_method}/{model_name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 736\n",
      "Shape of features in batch: torch.Size([8, 768])\n",
      "Shape of labels in batch: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset and dataloader\n",
    "dataset = method_to_split_to_dataset[feature_method][GOLD]\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "print(f\"Number of test examples: {len(dataset)}\")\n",
    "print(f\"Shape of features in batch: {next(iter(dataloader))[0].shape}\")\n",
    "print(f\"Shape of labels in batch: {next(iter(dataloader))[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaNN(\n",
       "  (sequence): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model_dir = f\"{models_dir}/{feature_method}/{model_name}.pt\"\n",
    "input_dim = [len(dataset[0][0])]\n",
    "init_params = input_dim + hidden_dims + output_dim\n",
    "model = VanillaNN(init_params)\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.1\n",
      "0.0: {'precision': 0.8773006134969326, 'recall': 0.6230936819172114, 'f1-score': 0.7286624203821657, 'support': 459}\n",
      "1.0: {'precision': 0.5780487804878048, 'recall': 0.855595667870036, 'f1-score': 0.6899563318777293, 'support': 277}\n",
      "accuracy: 0.7105978260869565\n",
      "\n",
      "Threshold=0.2\n",
      "0.0: {'precision': 0.8575581395348837, 'recall': 0.6427015250544662, 'f1-score': 0.734744707347447, 'support': 459}\n",
      "1.0: {'precision': 0.5816326530612245, 'recall': 0.8231046931407943, 'f1-score': 0.6816143497757847, 'support': 277}\n",
      "accuracy: 0.7105978260869565\n",
      "\n",
      "Threshold=0.3\n",
      "0.0: {'precision': 0.848314606741573, 'recall': 0.6579520697167756, 'f1-score': 0.7411042944785277, 'support': 459}\n",
      "1.0: {'precision': 0.5868421052631579, 'recall': 0.8050541516245487, 'f1-score': 0.6788432267884323, 'support': 277}\n",
      "accuracy: 0.7133152173913043\n",
      "\n",
      "Threshold=0.4\n",
      "0.0: {'precision': 0.8415300546448088, 'recall': 0.6710239651416122, 'f1-score': 0.7466666666666667, 'support': 459}\n",
      "1.0: {'precision': 0.5918918918918918, 'recall': 0.7906137184115524, 'f1-score': 0.676970633693972, 'support': 277}\n",
      "accuracy: 0.7160326086956522\n",
      "\n",
      "Threshold=0.5\n",
      "0.0: {'precision': 0.8320209973753281, 'recall': 0.690631808278867, 'f1-score': 0.7547619047619046, 'support': 459}\n",
      "1.0: {'precision': 0.6, 'recall': 0.7689530685920578, 'f1-score': 0.6740506329113924, 'support': 277}\n",
      "accuracy: 0.720108695652174\n",
      "\n",
      "Threshold=0.6\n",
      "0.0: {'precision': 0.8290155440414507, 'recall': 0.6971677559912854, 'f1-score': 0.7573964497041421, 'support': 459}\n",
      "1.0: {'precision': 0.6028571428571429, 'recall': 0.7617328519855595, 'f1-score': 0.6730462519936203, 'support': 277}\n",
      "accuracy: 0.7214673913043478\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on all thresholds\n",
    "metrics_string = \"\"\n",
    "for threshold in thresholds:\n",
    "    scores_dict = evaluate_model(model=model, dataloader=dataloader, confidence=threshold)\n",
    "    metrics_string += pretty_print_metrics(scores_dict, threshold) + \"\\n\"\n",
    "write_eval_to_file(file_path=eval_file, report_string=metrics_string)\n",
    "print(metrics_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw1_kernel",
   "language": "python",
   "name": "cw1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
