{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "Given CLIP-embedded text-image-pairs, there are multiple ways of feeding them to our model. For example, one could naively concatenate the text embedding with the image embedding of every pair and use those concatenations as input. We call the actual inputs \"features\". This notebook showcases different methods of feature extraction, given CLIP-embedded text-image-pairs. The resulting features of each method is stored in a pickle file.\n",
    "\n",
    "Additionally, in Section 1, we save the labels for every split in a separate pickle file.\n",
    "\n",
    "For example, the files for the method \"concat_cos\" from Section 4.1 are stored as follows.\n",
    "\n",
    "└── data/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── CT23_1A_checkworthy_multimodal_english_v1\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└── CT23_1A_checkworthy_multimodal_english_v2/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── labels/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── dev_labels_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── test_labels_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── train_labels_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── features/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── concat_cos/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── concat_cos_dev_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── concat_cos_test_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── concat_cos_train_v2.pickle\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports and Constants\n",
    "- Change the path to your dataset directory\n",
    "- Specify the dataset versions in the #CONSTANTS# part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "############## AUTORELOAD MAGIC ###################\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# FUNDAMENTAL MODULES\n",
    "import numpy as np\n",
    "\n",
    "#TASK-SPECIFIC MODULES\n",
    "import feature_extraction as fe\n",
    "import label_extraction as le\n",
    "import utils\n",
    "\n",
    "# CONSTANTS\n",
    "dataset_version = \"v2\"\n",
    "dataset_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_{dataset_version}\"\n",
    "pickled_labels_dir = f\"{dataset_dir}/labels\"\n",
    "train_embeddings_path = dataset_dir + f\"/embeddings_train_{dataset_version}.pickle\"\n",
    "dev_embeddings_path = dataset_dir + f\"/embeddings_dev_{dataset_version}.pickle\"\n",
    "test_embeddings_path = dataset_dir + f\"/embeddings_test_{dataset_version}.pickle\"\n",
    "\n",
    "# KEYS USED IN ALL DICTS\n",
    "from feature_extraction import TRAIN, DEV, TEST, TXT, IMG, SPLITS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:04.865788638Z",
     "start_time": "2023-05-16T09:56:04.429087584Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Extract and Pickle the Labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape label array: (2356,)\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/labels/train_labels_v2.pickle\n",
      "Shape label array: (271,)\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/labels/dev_labels_v2.pickle\n",
      "Shape label array: (548,)\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/labels/test_labels_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Initial JSON files of every split\n",
    "train_json = f\"{dataset_dir}/CT23_1A_checkworthy_multimodal_english_train.jsonl\"\n",
    "dev_json = f\"{dataset_dir}/CT23_1A_checkworthy_multimodal_english_dev.jsonl\"\n",
    "test_json = f\"{dataset_dir}/CT23_1A_checkworthy_multimodal_english_dev_test.jsonl\"\n",
    "all_splits_json = {\"train\": train_json, \"dev\": dev_json, \"test\": test_json}\n",
    "\n",
    "# Extract and pickle labels of every split\n",
    "for key, split in all_splits_json.items():\n",
    "      labels_array = le.get_labels_from_dataset(split)\n",
    "      print(f\"Shape label array: {labels_array.shape}\")\n",
    "      utils.pickle_features_or_labels(labels_array, f\"{pickled_labels_dir}/{key}_labels_{dataset_version}.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:04.905869145Z",
     "start_time": "2023-05-16T09:56:04.454115343Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Load Pickled Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For every split of the dataset, two embedding arrays (txt, img) are loaded. We will store those embeddings in a nested dictionary that is intuitive to reference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\ttxt\t\t\timg\n",
      "Tr\t\t(2356, 768)\t(2356, 768)\n",
      "De\t\t(271, 768)\t(271, 768)\n",
      "Te\t\t(548, 768)\t(548, 768)\n"
     ]
    }
   ],
   "source": [
    "# Two  arrays (txt, img) for every split\n",
    "train_txt_embeddings, train_img_embeddings = utils.get_embeddings_from_pickle_file(train_embeddings_path)\n",
    "dev_txt_embeddings, dev_img_embeddings = utils.get_embeddings_from_pickle_file(dev_embeddings_path)\n",
    "test_txt_embeddings, test_img_embeddings = utils.get_embeddings_from_pickle_file(test_embeddings_path)\n",
    "\n",
    "# Hold embeddings in a nested dictionary for easy referencing\n",
    "embeddings_dict = {\n",
    "      TRAIN: {TXT: train_txt_embeddings, IMG: train_img_embeddings},\n",
    "      DEV: {TXT: dev_txt_embeddings, IMG: dev_img_embeddings},\n",
    "      TEST:  {TXT: test_txt_embeddings, IMG: test_img_embeddings}\n",
    "}\n",
    "\n",
    "# Dimensions of embedding per split\n",
    "print(utils.table_embeddings_dims_per_split(embeddings_dict))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:04.946134286Z",
     "start_time": "2023-05-16T09:56:04.493460113Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Naive Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Concatenation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 1536)\n",
      "De\t\t(271, 1536)\n",
      "Te\t\t(548, 1536)\n"
     ]
    }
   ],
   "source": [
    "# Initialize to-be-pickled feature dictionary\n",
    "concat_features = {split: None for split in SPLITS}\n",
    "\n",
    "# Concatenate txt and img arrays of every split\n",
    "for split, embeddings in embeddings_dict.items():\n",
    "      features = np.concatenate((embeddings[TXT], embeddings[IMG]), axis=1)\n",
    "      concat_features[split] = features\n",
    "\n",
    "# Dimensions of input feature matrix per split\n",
    "print(utils.table_feature_dims_per_split(concat_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:04.970318608Z",
     "start_time": "2023-05-16T09:56:04.537270670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Txt emb: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Img emb: [ 0.20370999  0.39563796 -0.41939157 -0.35091972  0.02099419]\n",
      "Feature: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "\t\t [ 0.20370999  0.39563796 -0.41939157 -0.35091972  0.02099419]\n"
     ]
    }
   ],
   "source": [
    "# Spot check\n",
    "print(f\"Txt emb: {embeddings_dict[TRAIN][TXT][0][:5]}\")\n",
    "print(f\"Img emb: {embeddings_dict[TRAIN][IMG][0][-5:]}\")\n",
    "print(f\"Feature: {concat_features[TRAIN][0][:5]}\\n\"\n",
    "      f\"\\t\\t {concat_features[TRAIN][0][-5:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:04.988094250Z",
     "start_time": "2023-05-16T09:56:04.556045292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/concat/concat_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/concat/concat_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/concat/concat_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(concat_features, dataset_dir, feature_method=\"concat\",dataset_version=dataset_version)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.005581018Z",
     "start_time": "2023-05-16T09:56:04.571074492Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 768)\n",
      "De\t\t(271, 768)\n",
      "Te\t\t(548, 768)\n"
     ]
    }
   ],
   "source": [
    "# Initialize to-be-pickled feature dictionary\n",
    "sum_features = {split: None for split in SPLITS}\n",
    "\n",
    "# Sum txt and img matrices of every split\n",
    "for split, embeddings in embeddings_dict.items():\n",
    "      features = np.sum((embeddings[TXT], embeddings[IMG]), axis=0)\n",
    "      sum_features[split] = features\n",
    "\n",
    "# Shapes of input feature matrix per split\n",
    "print(utils.table_feature_dims_per_split(sum_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.015341813Z",
     "start_time": "2023-05-16T09:56:04.639664110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Txt emb: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Img emb: [-0.24858032  0.6837659   0.81424457  0.59864545  0.49090493]\n",
      "Feature: [ 0.06400155  1.5459961   0.61851794  1.0155458  -0.33965725]\n"
     ]
    }
   ],
   "source": [
    "# Spot check\n",
    "print(f\"Txt emb: {embeddings_dict[TRAIN][TXT][0][:5]}\")\n",
    "print(f\"Img emb: {embeddings_dict[TRAIN][IMG][0][:5]}\")\n",
    "print(f\"Feature: {sum_features[TRAIN][0][:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.015832145Z",
     "start_time": "2023-05-16T09:56:04.641348834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/sum/sum_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/sum/sum_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/sum/sum_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(sum_features, dataset_dir, feature_method=\"sum\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.016155636Z",
     "start_time": "2023-05-16T09:56:04.686331041Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 768)\n",
      "De\t\t(271, 768)\n",
      "Te\t\t(548, 768)\n"
     ]
    }
   ],
   "source": [
    "# Initialize to-be-pickled feature dictionary\n",
    "mean_features = {split: None for split in SPLITS}\n",
    "\n",
    "# Compute the mean of txt and img matrices of every split\n",
    "for split, embeddings in embeddings_dict.items():\n",
    "      features = np.mean((embeddings[TXT], embeddings[IMG]), axis=0)\n",
    "      mean_features[split] = features\n",
    "\n",
    "# Shapes of input feature matrix per split\n",
    "print(utils.table_feature_dims_per_split(mean_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.016588061Z",
     "start_time": "2023-05-16T09:56:04.686517466Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Txt emb: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Img emb: [-0.24858032  0.6837659   0.81424457  0.59864545  0.49090493]\n",
      "Feature: [ 0.03200077  0.77299803  0.30925897  0.5077729  -0.16982862]\n"
     ]
    }
   ],
   "source": [
    "# Spot check\n",
    "print(f\"Txt emb: {embeddings_dict[TRAIN][TXT][0][:5]}\")\n",
    "print(f\"Img emb: {embeddings_dict[TRAIN][IMG][0][:5]}\")\n",
    "print(f\"Feature: {mean_features[TRAIN][0][:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.016886286Z",
     "start_time": "2023-05-16T09:56:04.732998297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/mean/mean_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/mean/mean_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/mean/mean_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(mean_features, dataset_dir, feature_method=\"mean\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.017212607Z",
     "start_time": "2023-05-16T09:56:04.733191631Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Hadamard Product"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 768)\n",
      "De\t\t(271, 768)\n",
      "Te\t\t(548, 768)\n"
     ]
    }
   ],
   "source": [
    "# Initialize to-be-pickled feature dictionary\n",
    "hadamard_features = {split: None for split in SPLITS}\n",
    "\n",
    "# Sum txt and img matrices of every split\n",
    "for split, embeddings in embeddings_dict.items():\n",
    "      features = np.multiply(embeddings[TXT], embeddings[IMG])\n",
    "      hadamard_features[split] = features\n",
    "\n",
    "# Shapes of input feature matrix per split\n",
    "print(utils.table_feature_dims_per_split(hadamard_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.017517954Z",
     "start_time": "2023-05-16T09:56:04.779671062Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Txt emb: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Img emb: [-0.24858032  0.6837659   0.81424457  0.59864545  0.49090493]\n",
      "Feature: [-0.0777017   0.5895636  -0.15936933  0.24957554 -0.40772706]\n"
     ]
    }
   ],
   "source": [
    "# Spot check\n",
    "print(f\"Txt emb: {embeddings_dict[TRAIN][TXT][0][:5]}\")\n",
    "print(f\"Img emb: {embeddings_dict[TRAIN][IMG][0][:5]}\")\n",
    "print(f\"Feature: {hadamard_features[TRAIN][0][:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.017831944Z",
     "start_time": "2023-05-16T09:56:04.779814165Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/hadamard/hadamard_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/hadamard/hadamard_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/hadamard/hadamard_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(hadamard_features, dataset_dir, feature_method=\"hadamard\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.018089594Z",
     "start_time": "2023-05-16T09:56:04.823099982Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Naive Approaches Combined with Text/Image Similarity\n",
    "All the feature matrices from above can be supplemented with an additional feature dimension that captures the cosine similarity of every text-image-embedding-pair. It is the same for every method. Hence, we will compute the cosine similarity dimension once."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Cosine sim of a single embedding with itself\n",
    "print(fe.cosine(embeddings_dict[TRAIN][TXT][0], embeddings_dict[TRAIN][TXT][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.018372923Z",
     "start_time": "2023-05-16T09:56:04.823379067Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.99999994 1.0000001  ... 1.         1.         0.99999994]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Compute cosine array for a split with itself\n",
    "print(fe.compute_cosine_array(embeddings_dict[TRAIN][TXT], embeddings_dict[TRAIN][TXT]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.020175632Z",
     "start_time": "2023-05-16T09:56:04.866406888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356,)\n",
      "De\t\t(271,)\n",
      "Te\t\t(548,)\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity dimension for every split\n",
    "train_cosine_array = fe.compute_cosine_array(embeddings_dict[TRAIN][TXT], embeddings_dict[TRAIN][IMG])\n",
    "dev_cosine_array = fe.compute_cosine_array(embeddings_dict[DEV][TXT], embeddings_dict[DEV][IMG])\n",
    "test_cosine_array = fe.compute_cosine_array(embeddings_dict[TEST][TXT], embeddings_dict[TEST][IMG])\n",
    "\n",
    "# Save cosine sim dimension in a dict for easy referencing\n",
    "cosine_dict = {\n",
    "      TRAIN: train_cosine_array,\n",
    "      DEV: dev_cosine_array,\n",
    "      TEST:  test_cosine_array\n",
    "}\n",
    "\n",
    "# Check cosine sim shapes\n",
    "print(utils.table_feature_dims_per_split(cosine_dict))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.220451508Z",
     "start_time": "2023-05-16T09:56:04.866607311Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Concatenation + Cosine Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 1537)\n",
      "De\t\t(271, 1537)\n",
      "Te\t\t(548, 1537)\n"
     ]
    }
   ],
   "source": [
    "# Add cosine dim to every split\n",
    "concat_cos_features = fe.add_feature_dim_to_all_splits(concat_features, cosine_dict)\n",
    "print(utils.table_feature_dims_per_split(concat_cos_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.221172433Z",
     "start_time": "2023-05-16T09:56:04.912992429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/concat_cos/concat_cos_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/concat_cos/concat_cos_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/concat_cos/concat_cos_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(concat_cos_features, dataset_dir, feature_method=\"concat_cos\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.234164828Z",
     "start_time": "2023-05-16T09:56:04.942582733Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Sum + Cosine Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 769)\n",
      "De\t\t(271, 769)\n",
      "Te\t\t(548, 769)\n"
     ]
    }
   ],
   "source": [
    "# Add cosine dim to every split\n",
    "sum_cos_features = fe.add_feature_dim_to_all_splits(sum_features, cosine_dict)\n",
    "print(utils.table_feature_dims_per_split(sum_cos_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.251538506Z",
     "start_time": "2023-05-16T09:56:05.026354814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/sum_cos/sum_cos_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/sum_cos/sum_cos_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/sum_cos/sum_cos_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(sum_cos_features, dataset_dir, feature_method=\"sum_cos\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.252472627Z",
     "start_time": "2023-05-16T09:56:05.026495317Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Mean + Cosine Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 769)\n",
      "De\t\t(271, 769)\n",
      "Te\t\t(548, 769)\n"
     ]
    }
   ],
   "source": [
    "# Add cosine dim to every split\n",
    "mean_cos_features = fe.add_feature_dim_to_all_splits(mean_features, cosine_dict)\n",
    "print(utils.table_feature_dims_per_split(mean_cos_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.252661687Z",
     "start_time": "2023-05-16T09:56:05.069836671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/mean_cos/mean_cos_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/mean_cos/mean_cos_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/mean_cos/mean_cos_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(mean_cos_features, dataset_dir, feature_method=\"mean_cos\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.253160188Z",
     "start_time": "2023-05-16T09:56:05.070073066Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Hadamard Product + Cosine Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tShape\n",
      "Tr\t\t(2356, 769)\n",
      "De\t\t(271, 769)\n",
      "Te\t\t(548, 769)\n"
     ]
    }
   ],
   "source": [
    "# Add cosine dim to every split\n",
    "hadamard_cos_features = fe.add_feature_dim_to_all_splits(hadamard_features, cosine_dict)\n",
    "print(utils.table_feature_dims_per_split(hadamard_cos_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.253340458Z",
     "start_time": "2023-05-16T09:56:05.113070242Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/hadamard_cos/hadamard_cos_train_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/hadamard_cos/hadamard_cos_dev_v2.pickle\n",
      "Pickled: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/features/hadamard_cos/hadamard_cos_test_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "# Pickle the feature matrix of every split in its own file\n",
    "utils.pickle_all_splits(hadamard_cos_features, dataset_dir, feature_method=\"hadamard_cos\", dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.254217908Z",
     "start_time": "2023-05-16T09:56:05.113305479Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Storage\n",
    "Every presented feature engineering method yields three files, one for every split. For example, the files for the concat_cos are stored as follows.\n",
    ".\n",
    "└── data/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── CT23_1A_checkworthy_multimodal_english_v1\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└── CT23_1A_checkworthy_multimodal_english_v2/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── labels/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── dev_labels_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── test_labels_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── train_labels_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── features/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── concat_cos/\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── concat_cos_dev_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── concat_cos_test_v2.pickle\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── concat_cos_train_v2.pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Load pickled features and labels\n",
    "This section shows how to load a pickled feature matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2356, 769)\n"
     ]
    }
   ],
   "source": [
    "# Example: Load the mean_cos feature matrix of the train split\n",
    "file_path = f\"{dataset_dir}/features/mean_cos/mean_cos_train_{dataset_version}.pickle\"\n",
    "train_mean_cos_features = np.load(file_path, allow_pickle=True)\n",
    "print(type(train_mean_cos_features))\n",
    "print(train_mean_cos_features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.254379632Z",
     "start_time": "2023-05-16T09:56:05.156363562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2356,)\n"
     ]
    }
   ],
   "source": [
    "# Example: Load the train labels\n",
    "file_path = f\"{dataset_dir}/labels/train_labels_{dataset_version}.pickle\"\n",
    "labels = np.load(file_path, allow_pickle=True)\n",
    "print(type(labels))\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T09:56:05.254554211Z",
     "start_time": "2023-05-16T09:56:05.156579050Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Further Feature Engineering Methods\n",
    "For future approaches beyond the baseline, collect  more subtle feature engineering methods here:\n",
    "- RpBERT: https://ar5iv.labs.arxiv.org/html/2102.02967"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cw1_kernel",
   "language": "python",
   "display_name": "cw1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
