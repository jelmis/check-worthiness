{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Notebook Summary\n",
    "- Get BERTweet from huggingface\n",
    "- Use BERTweet to\n",
    "    - extract embeddings from Tweets\n",
    "    - extract embeddings from concatenations of Tweet + OCR text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0. Imports and Constants\n",
    "- Do not forget to select dataset version in the #CONSTANTS# part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T18:07:32.321626748Z",
     "start_time": "2023-07-09T18:07:32.253071738Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "############## AUTORELOAD MAGIC ###################\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "###################################################\n",
    "\n",
    "############## FUNDAMENTAL MODULES ################\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "##################################################\n",
    "\n",
    "############## TASK-SPECIFIC MODULES #############\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from data import TweetNormalizer, utils, feature_extraction\n",
    "###################################################\n",
    "\n",
    "############## DATA SCIENCE & ML MODULES ##########\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy import stats\n",
    "###################################################\n",
    "\n",
    "############# CONSTANT DICT KEYS ###################\n",
    "# Constant dict keys\n",
    "TRAIN = \"train\"\n",
    "DEV = \"dev\"\n",
    "TEST = \"test\"\n",
    "GOLD = \"gold\"\n",
    "TXT = \"txt\"\n",
    "IMG = \"img\"\n",
    "OCR = \"ocr\"\n",
    "TXT_OCR = \"txt_ocr\"\n",
    "SPLITS = [TRAIN, DEV, TEST, GOLD]\n",
    "####################################################\n",
    "\n",
    "####################### SELECT ###########################\n",
    "users = [\"patriziopalmisano\", \"onurdenizguler\", \"jockl\"]\n",
    "user = users[2] # SELECT USER\n",
    "version = \"v2\" # SELECT DATASET VERSION\n",
    "dataset_version = version\n",
    "##########################################################\n",
    "\n",
    "if user in users[:2]:\n",
    "    cw_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive\"\n",
    "    data_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english\"\n",
    "    data_dir_with_version = f\"{data_dir}_{dataset_version}\"\n",
    "    gold_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english_test_gold\"\n",
    "\n",
    "else:\n",
    "    cw_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive\"\n",
    "    data_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english\"\n",
    "    data_dir_with_version = f\"{data_dir}_{dataset_version}\"\n",
    "    gold_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english_test_gold\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Load all Datasets\n",
    "First, we extract all the raw texts from the JSON files. Note that the concatenation of tweet and OCR text is realized with a new line in-between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of txt, img, ocr, txt+ocr arrays in train, test, dev, gold:\n",
      "2356 2356 2356 2356\n",
      "271 271 2356 2356\n",
      "548 548 2356 2356\n",
      "736 736 2356 2356\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "raw_dataset, tweet_texts, imgs, tweet_ids, ocr_texts, tweet_concat_ocr = utils.load_data_splits_with_gold_dataset(data_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "\n",
      "OCR:\n",
      "CORONAVIRUS\n",
      "MGN\n",
      "\n",
      "\n",
      "Concat:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "CORONAVIRUS\n",
      "MGN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect Tweet and OCR concatenation\n",
    "print(f\"Tweet:\\n{tweet_texts[TRAIN][6]}\")\n",
    "print(f\"\\nOCR:\\n{ocr_texts[TRAIN][6]}\")\n",
    "print(f\"\\nConcat:\\n{tweet_concat_ocr[TRAIN][6]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Normalize Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n",
      "2356\n"
     ]
    }
   ],
   "source": [
    "# Normalize all tweets using TweetNormalizer()\n",
    "normalized_tweets = {split: [TweetNormalizer.normalizeTweet(tweet) for tweet in tweet_texts[split]] for split in SPLITS}\n",
    "normalized_tweet_concat_ocr = {split: [TweetNormalizer.normalizeTweet(concat) for concat in tweet_concat_ocr[split]] for split in SPLITS}\n",
    "print(len(normalized_tweets[TRAIN]))\n",
    "print(len(normalized_tweet_concat_ocr[TRAIN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "\n",
      "Normalized Tweet:\n",
      "Despite calls for calm , some local people are panicking over the deadly coronavirus outbreak . \" I really am . It 's really something that I 'm really frightened about right now . \" \" It might be actually worse than what they 're telling us . \" HTTPURL HTTPURL\n"
     ]
    }
   ],
   "source": [
    "# Inspect normalization of tweets\n",
    "print(f\"Tweet:\\n{tweet_texts[TRAIN][6]}\")\n",
    "print(f\"\\nNormalized Tweet:\\n{normalized_tweets[TRAIN][6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "Normalized Concat:\n",
      "Despite calls for calm , some local people are panicking over the deadly coronavirus outbreak . \" I really am . It 's really something that I 'm really frightened about right now . \" \" It might be actually worse than what they 're telling us . \" HTTPURL HTTPURL CORONAVIRUS MGN\n"
     ]
    }
   ],
   "source": [
    "# Inspect normalization of tweet_ocr_concat\n",
    "print(f\"Concat:\\n{tweet_texts[TRAIN][6]}\")\n",
    "print(f\"Normalized Concat:\\n{normalized_tweet_concat_ocr[TRAIN][6]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set Up BERTweet and Embed Minimal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up devicde\n",
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "         (\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago Cardinal : Global Warming , Migrants Are ‘ Bigger Agenda ' than Sex Abuse HTTPURL via @USER HTTPURL\n",
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "# Encode one example text\n",
    "input_text = normalized_tweets[TRAIN][1]\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)])\n",
    "print(input_text)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Embed the example\n",
    "with torch.no_grad():\n",
    "    features = bertweet(input_ids)\n",
    "print(features.pooler_output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resolve the Sequence Length Issue\n",
    "Maximum number of tokens for BERTweet input: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token limit and padding ID\n",
    "token_limit = 128\n",
    "padding_idx = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the blunt attempt of tokenizing yields the warning that BERTweet only accepts input sequences of length < 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# First, tokenize everything\n",
    "normalized_tweets_tokenized = {split: tokenizer(normalized_tweets[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}\n",
    "normalized_concat_tokenized = {split: tokenizer(normalized_tweet_concat_ocr[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the plain tweets never exceed the token limit, at least one sample from the tweet+OCR concatenations conatains 3586 tokens. This results in padding any sequence to this number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2356, 116])\n",
      "torch.Size([2356, 3586])\n"
     ]
    }
   ],
   "source": [
    "# Inspect tokenized samples\n",
    "print(normalized_tweets_tokenized[TRAIN].shape)\n",
    "print(normalized_concat_tokenized[TRAIN].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which samples exactly feature excess tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ID, Length, Excess Tokens)\n",
      "Tweets: 0\t[]\n",
      "Concat: 296\t[(651, 129, 1), (1144, 129, 1), (1985, 129, 1), (526, 130, 2), (1410, 130, 2), (1454, 130, 2), (1738, 130, 2), (1338, 131, 3), (1978, 131, 3), (769, 132, 4), (1615, 132, 4), (67, 133, 5), (471, 133, 5), (1070, 133, 5), (1136, 133, 5), (1519, 133, 5), (1984, 133, 5), (1991, 133, 5), (330, 134, 6), (367, 134, 6), (1715, 134, 6), (2007, 134, 6), (120, 135, 7), (2045, 135, 7), (2205, 135, 7), (318, 136, 8), (672, 136, 8), (137, 137, 9), (1770, 137, 9), (279, 138, 10), (1053, 138, 10), (1429, 138, 10), (59, 139, 11), (762, 139, 11), (1902, 139, 11), (830, 140, 12), (2130, 140, 12), (1097, 141, 13), (1821, 141, 13), (785, 142, 14), (1700, 142, 14), (2290, 142, 14), (215, 143, 15), (1408, 143, 15), (1710, 144, 16), (2006, 144, 16), (631, 148, 20), (1425, 149, 21), (1956, 149, 21), (2138, 149, 21), (109, 150, 22), (1026, 150, 22), (1780, 150, 22), (371, 151, 23), (862, 151, 23), (2016, 151, 23), (2222, 151, 23), (657, 152, 24), (51, 153, 25), (385, 153, 25), (704, 153, 25), (766, 153, 25), (1448, 153, 25), (1899, 153, 25), (1039, 155, 27), (2325, 155, 27), (174, 157, 29), (715, 157, 29), (747, 157, 29), (1388, 157, 29), (89, 159, 31), (347, 159, 31), (713, 159, 31), (1651, 159, 31), (1714, 159, 31), (772, 160, 32), (1895, 162, 34), (2157, 162, 34), (490, 163, 35), (13, 164, 36), (681, 164, 36), (1221, 164, 36), (2046, 164, 36), (2165, 164, 36), (0, 165, 37), (231, 165, 37), (1276, 165, 37), (2288, 166, 38), (1854, 167, 39), (2256, 167, 39), (453, 168, 40), (537, 169, 41), (55, 170, 42), (2224, 170, 42), (462, 171, 43), (635, 172, 44), (1758, 172, 44), (1509, 173, 45), (225, 174, 46), (767, 174, 46), (1261, 174, 46), (1302, 175, 47), (472, 177, 49), (606, 177, 49), (1379, 177, 49), (300, 178, 50), (358, 178, 50), (1248, 178, 50), (1496, 178, 50), (321, 179, 51), (1778, 179, 51), (2354, 179, 51), (236, 180, 52), (1278, 180, 52), (2177, 180, 52), (1151, 181, 53), (1427, 181, 53), (150, 182, 54), (2076, 182, 54), (987, 183, 55), (2301, 183, 55), (1824, 184, 56), (2061, 184, 56), (1212, 185, 57), (1385, 185, 57), (1350, 186, 58), (690, 187, 59), (1659, 187, 59), (1771, 187, 59), (2079, 187, 59), (167, 188, 60), (1655, 188, 60), (1663, 188, 60), (764, 189, 61), (1352, 189, 61), (1523, 190, 62), (1560, 191, 63), (1860, 191, 63), (1981, 192, 64), (708, 193, 65), (1474, 193, 65), (2204, 193, 65), (476, 194, 66), (143, 195, 67), (1317, 196, 68), (1106, 198, 70), (2114, 199, 71), (295, 200, 72), (811, 200, 72), (1995, 201, 73), (2355, 201, 73), (675, 203, 75), (1092, 203, 75), (2044, 203, 75), (365, 204, 76), (1399, 204, 76), (679, 205, 77), (1722, 205, 77), (2162, 205, 77), (2282, 205, 77), (863, 206, 78), (953, 210, 82), (1912, 213, 85), (204, 214, 86), (76, 215, 87), (976, 215, 87), (2324, 215, 87), (784, 216, 88), (820, 218, 90), (1348, 218, 90), (15, 219, 91), (661, 219, 91), (1022, 221, 93), (1888, 222, 94), (2160, 223, 95), (827, 224, 96), (1467, 224, 96), (1090, 225, 97), (163, 228, 100), (1198, 228, 100), (1840, 228, 100), (1717, 234, 106), (2317, 235, 107), (735, 236, 108), (1314, 236, 108), (1593, 236, 108), (1142, 237, 109), (1660, 238, 110), (1456, 240, 112), (1936, 240, 112), (2233, 241, 113), (659, 243, 115), (1801, 248, 120), (2209, 248, 120), (1863, 251, 123), (1734, 253, 125), (1346, 256, 128), (1977, 256, 128), (257, 258, 130), (356, 259, 131), (2161, 259, 131), (2262, 259, 131), (1190, 260, 132), (1326, 262, 134), (406, 268, 140), (1883, 272, 144), (1368, 274, 146), (776, 277, 149), (505, 278, 150), (79, 279, 151), (1363, 280, 152), (1884, 280, 152), (1749, 281, 153), (34, 282, 154), (1445, 283, 155), (296, 285, 157), (1239, 288, 160), (1943, 288, 160), (463, 292, 164), (709, 296, 168), (9, 297, 169), (415, 297, 169), (155, 298, 170), (1958, 300, 172), (1587, 303, 175), (967, 304, 176), (1024, 304, 176), (408, 306, 178), (1224, 307, 179), (1699, 309, 181), (1312, 311, 183), (826, 313, 185), (165, 316, 188), (2219, 316, 188), (600, 318, 190), (1761, 318, 190), (945, 319, 191), (260, 322, 194), (1229, 332, 204), (731, 334, 206), (2066, 335, 207), (2051, 338, 210), (955, 340, 212), (2146, 346, 218), (528, 349, 221), (625, 349, 221), (8, 350, 222), (2126, 350, 222), (1184, 351, 223), (1736, 353, 225), (1799, 356, 228), (1113, 357, 229), (1727, 372, 244), (27, 379, 251), (382, 383, 255), (1325, 393, 265), (2307, 393, 265), (2239, 395, 267), (823, 399, 271), (1127, 399, 271), (1604, 416, 288), (1543, 419, 291), (1624, 419, 291), (749, 422, 294), (2017, 436, 308), (130, 445, 317), (744, 447, 319), (2133, 447, 319), (753, 452, 324), (918, 465, 337), (2021, 471, 343), (1518, 485, 357), (618, 503, 375), (1025, 517, 389), (861, 519, 391), (1139, 528, 400), (1105, 545, 417), (2186, 563, 435), (574, 573, 445), (2057, 581, 453), (1622, 584, 456), (737, 589, 461), (1461, 617, 489), (2302, 679, 551), (1079, 682, 554), (1403, 702, 574), (86, 704, 576), (502, 751, 623), (2291, 849, 721), (2008, 907, 779), (5, 908, 780), (556, 1051, 923), (1594, 1250, 1122), (2321, 1275, 1147), (1628, 1620, 1492), (1698, 3586, 3458)]\n"
     ]
    }
   ],
   "source": [
    "# Find all the examples with excess tokens\n",
    "split_to_tweets_with_excess_tokens = utils.get_samples_with_excess_tokens(token_limit, normalized_tweets_tokenized, padding_idx)\n",
    "split_to_concat_with_excess_tokens = utils.get_samples_with_excess_tokens(token_limit, normalized_concat_tokenized, padding_idx)\n",
    "print(f\"(ID, Length, Excess Tokens)\")\n",
    "print(f\"Tweets: {len(split_to_tweets_with_excess_tokens[TRAIN])}\\t{split_to_tweets_with_excess_tokens[TRAIN]}\")\n",
    "print(f\"Concat: {len(split_to_concat_with_excess_tokens[TRAIN])}\\t{split_to_concat_with_excess_tokens[TRAIN]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst case: We truncate 296 examples (for the train set) from the tweet+OCR concatenations. Not a lot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now further normalize the samples that feature excess tokens. If this further normalizations still yields a sequence that is too long, the following function automatically truncates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further normalize/truncate those examples with excess tokens\n",
    "final_normalized_tweets = utils.further_normalize_samples_with_excess_tokens(token_limit, normalized_tweets, split_to_tweets_with_excess_tokens, normalized_tweets_tokenized, tokenizer, padding_idx)\n",
    "final_normalized_concat = utils.further_normalize_samples_with_excess_tokens(token_limit, normalized_tweet_concat_ocr, split_to_concat_with_excess_tokens, normalized_concat_tokenized, tokenizer, padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode final texts\n",
    "tweets_encoded = {split: tokenizer(final_normalized_tweets[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}\n",
    "concat_encoded = {split: tokenizer(final_normalized_concat[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: torch.Size([2356, 116])\ttorch.Size([2356, 116])\n",
      "Concat: torch.Size([2356, 3586])\ttorch.Size([2356, 128])\n"
     ]
    }
   ],
   "source": [
    "# Inspect tokenized split before/after normalization and truncation\n",
    "print(f\"Tweet: {normalized_tweets_tokenized[TRAIN].shape}\\t{tweets_encoded[TRAIN].shape}\")\n",
    "print(f\"Concat: {normalized_concat_tokenized[TRAIN].shape}\\t{concat_encoded[TRAIN].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now, the concat sequences are padded only up to BERTweet's limit of 128."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Embed the Tokenized Samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the batch size with which BERTweet should embed our encoded data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size here\n",
    "batch_size = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Embed the Tweets Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train\n",
      "Num samples: 2356\n",
      "Num batches: 294\n",
      "train batch 0/294\n",
      "train batch 1/294\n",
      "train batch 2/294\n",
      "train batch 3/294\n",
      "train batch 4/294\n",
      "train batch 5/294\n",
      "train batch 6/294\n",
      "train batch 7/294\n",
      "train batch 8/294\n",
      "train batch 9/294\n",
      "train batch 10/294\n",
      "train batch 11/294\n",
      "train batch 12/294\n",
      "train batch 13/294\n",
      "train batch 14/294\n",
      "train batch 15/294\n",
      "train batch 16/294\n",
      "train batch 17/294\n",
      "train batch 18/294\n",
      "train batch 19/294\n",
      "train batch 20/294\n",
      "train batch 21/294\n",
      "train batch 22/294\n",
      "train batch 23/294\n",
      "train batch 24/294\n",
      "train batch 25/294\n",
      "train batch 26/294\n",
      "train batch 27/294\n",
      "train batch 28/294\n",
      "train batch 29/294\n",
      "train batch 30/294\n",
      "train batch 31/294\n",
      "train batch 32/294\n",
      "train batch 33/294\n",
      "train batch 34/294\n",
      "train batch 35/294\n",
      "train batch 36/294\n",
      "train batch 37/294\n",
      "train batch 38/294\n",
      "train batch 39/294\n",
      "train batch 40/294\n",
      "train batch 41/294\n",
      "train batch 42/294\n",
      "train batch 43/294\n",
      "train batch 44/294\n",
      "train batch 45/294\n",
      "train batch 46/294\n",
      "train batch 47/294\n",
      "train batch 48/294\n",
      "train batch 49/294\n",
      "train batch 50/294\n",
      "train batch 51/294\n",
      "train batch 52/294\n",
      "train batch 53/294\n",
      "train batch 54/294\n",
      "train batch 55/294\n",
      "train batch 56/294\n",
      "train batch 57/294\n",
      "train batch 58/294\n",
      "train batch 59/294\n",
      "train batch 60/294\n",
      "train batch 61/294\n",
      "train batch 62/294\n",
      "train batch 63/294\n",
      "train batch 64/294\n",
      "train batch 65/294\n",
      "train batch 66/294\n",
      "train batch 67/294\n",
      "train batch 68/294\n",
      "train batch 69/294\n",
      "train batch 70/294\n",
      "train batch 71/294\n",
      "train batch 72/294\n",
      "train batch 73/294\n",
      "train batch 74/294\n",
      "train batch 75/294\n",
      "train batch 76/294\n",
      "train batch 77/294\n",
      "train batch 78/294\n",
      "train batch 79/294\n",
      "train batch 80/294\n",
      "train batch 81/294\n",
      "train batch 82/294\n",
      "train batch 83/294\n",
      "train batch 84/294\n",
      "train batch 85/294\n",
      "train batch 86/294\n",
      "train batch 87/294\n",
      "train batch 88/294\n",
      "train batch 89/294\n",
      "train batch 90/294\n",
      "train batch 91/294\n",
      "train batch 92/294\n",
      "train batch 93/294\n",
      "train batch 94/294\n",
      "train batch 95/294\n",
      "train batch 96/294\n",
      "train batch 97/294\n",
      "train batch 98/294\n",
      "train batch 99/294\n",
      "train batch 100/294\n",
      "train batch 101/294\n",
      "train batch 102/294\n",
      "train batch 103/294\n",
      "train batch 104/294\n",
      "train batch 105/294\n",
      "train batch 106/294\n",
      "train batch 107/294\n",
      "train batch 108/294\n",
      "train batch 109/294\n",
      "train batch 110/294\n",
      "train batch 111/294\n",
      "train batch 112/294\n",
      "train batch 113/294\n",
      "train batch 114/294\n",
      "train batch 115/294\n",
      "train batch 116/294\n",
      "train batch 117/294\n",
      "train batch 118/294\n",
      "train batch 119/294\n",
      "train batch 120/294\n",
      "train batch 121/294\n",
      "train batch 122/294\n",
      "train batch 123/294\n",
      "train batch 124/294\n",
      "train batch 125/294\n",
      "train batch 126/294\n",
      "train batch 127/294\n",
      "train batch 128/294\n",
      "train batch 129/294\n",
      "train batch 130/294\n",
      "train batch 131/294\n",
      "train batch 132/294\n",
      "train batch 133/294\n",
      "train batch 134/294\n",
      "train batch 135/294\n",
      "train batch 136/294\n",
      "train batch 137/294\n",
      "train batch 138/294\n",
      "train batch 139/294\n",
      "train batch 140/294\n",
      "train batch 141/294\n",
      "train batch 142/294\n",
      "train batch 143/294\n",
      "train batch 144/294\n",
      "train batch 145/294\n",
      "train batch 146/294\n",
      "train batch 147/294\n",
      "train batch 148/294\n",
      "train batch 149/294\n",
      "train batch 150/294\n",
      "train batch 151/294\n",
      "train batch 152/294\n",
      "train batch 153/294\n",
      "train batch 154/294\n",
      "train batch 155/294\n",
      "train batch 156/294\n",
      "train batch 157/294\n",
      "train batch 158/294\n",
      "train batch 159/294\n",
      "train batch 160/294\n",
      "train batch 161/294\n",
      "train batch 162/294\n",
      "train batch 163/294\n",
      "train batch 164/294\n",
      "train batch 165/294\n",
      "train batch 166/294\n",
      "train batch 167/294\n",
      "train batch 168/294\n",
      "train batch 169/294\n",
      "train batch 170/294\n",
      "train batch 171/294\n",
      "train batch 172/294\n",
      "train batch 173/294\n",
      "train batch 174/294\n",
      "train batch 175/294\n",
      "train batch 176/294\n",
      "train batch 177/294\n",
      "train batch 178/294\n",
      "train batch 179/294\n",
      "train batch 180/294\n",
      "train batch 181/294\n",
      "train batch 182/294\n",
      "train batch 183/294\n",
      "train batch 184/294\n",
      "train batch 185/294\n",
      "train batch 186/294\n",
      "train batch 187/294\n",
      "train batch 188/294\n",
      "train batch 189/294\n",
      "train batch 190/294\n",
      "train batch 191/294\n",
      "train batch 192/294\n",
      "train batch 193/294\n",
      "train batch 194/294\n",
      "train batch 195/294\n",
      "train batch 196/294\n",
      "train batch 197/294\n",
      "train batch 198/294\n",
      "train batch 199/294\n",
      "train batch 200/294\n",
      "train batch 201/294\n",
      "train batch 202/294\n",
      "train batch 203/294\n",
      "train batch 204/294\n",
      "train batch 205/294\n",
      "train batch 206/294\n",
      "train batch 207/294\n",
      "train batch 208/294\n",
      "train batch 209/294\n",
      "train batch 210/294\n",
      "train batch 211/294\n",
      "train batch 212/294\n",
      "train batch 213/294\n",
      "train batch 214/294\n",
      "train batch 215/294\n",
      "train batch 216/294\n",
      "train batch 217/294\n",
      "train batch 218/294\n",
      "train batch 219/294\n",
      "train batch 220/294\n",
      "train batch 221/294\n",
      "train batch 222/294\n",
      "train batch 223/294\n",
      "train batch 224/294\n",
      "train batch 225/294\n",
      "train batch 226/294\n",
      "train batch 227/294\n",
      "train batch 228/294\n",
      "train batch 229/294\n",
      "train batch 230/294\n",
      "train batch 231/294\n",
      "train batch 232/294\n",
      "train batch 233/294\n",
      "train batch 234/294\n",
      "train batch 235/294\n",
      "train batch 236/294\n",
      "train batch 237/294\n",
      "train batch 238/294\n",
      "train batch 239/294\n",
      "train batch 240/294\n",
      "train batch 241/294\n",
      "train batch 242/294\n",
      "train batch 243/294\n",
      "train batch 244/294\n",
      "train batch 245/294\n",
      "train batch 246/294\n",
      "train batch 247/294\n",
      "train batch 248/294\n",
      "train batch 249/294\n",
      "train batch 250/294\n",
      "train batch 251/294\n",
      "train batch 252/294\n",
      "train batch 253/294\n",
      "train batch 254/294\n",
      "train batch 255/294\n",
      "train batch 256/294\n",
      "train batch 257/294\n",
      "train batch 258/294\n",
      "train batch 259/294\n",
      "train batch 260/294\n",
      "train batch 261/294\n",
      "train batch 262/294\n",
      "train batch 263/294\n",
      "train batch 264/294\n",
      "train batch 265/294\n",
      "train batch 266/294\n",
      "train batch 267/294\n",
      "train batch 268/294\n",
      "train batch 269/294\n",
      "train batch 270/294\n",
      "train batch 271/294\n",
      "train batch 272/294\n",
      "train batch 273/294\n",
      "train batch 274/294\n",
      "train batch 275/294\n",
      "train batch 276/294\n",
      "train batch 277/294\n",
      "train batch 278/294\n",
      "train batch 279/294\n",
      "train batch 280/294\n",
      "train batch 281/294\n",
      "train batch 282/294\n",
      "train batch 283/294\n",
      "train batch 284/294\n",
      "train batch 285/294\n",
      "train batch 286/294\n",
      "train batch 287/294\n",
      "train batch 288/294\n",
      "train batch 289/294\n",
      "train batch 290/294\n",
      "train batch 291/294\n",
      "train batch 292/294\n",
      "train batch 293/294\n",
      "train batch 294/294\n",
      "\n",
      "BERTweet_embeddings_train: torch.Size([2356, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_train.pickle\n",
      "Split: dev\n",
      "Num samples: 271\n",
      "Num batches: 33\n",
      "dev batch 0/33\n",
      "dev batch 1/33\n",
      "dev batch 2/33\n",
      "dev batch 3/33\n",
      "dev batch 4/33\n",
      "dev batch 5/33\n",
      "dev batch 6/33\n",
      "dev batch 7/33\n",
      "dev batch 8/33\n",
      "dev batch 9/33\n",
      "dev batch 10/33\n",
      "dev batch 11/33\n",
      "dev batch 12/33\n",
      "dev batch 13/33\n",
      "dev batch 14/33\n",
      "dev batch 15/33\n",
      "dev batch 16/33\n",
      "dev batch 17/33\n",
      "dev batch 18/33\n",
      "dev batch 19/33\n",
      "dev batch 20/33\n",
      "dev batch 21/33\n",
      "dev batch 22/33\n",
      "dev batch 23/33\n",
      "dev batch 24/33\n",
      "dev batch 25/33\n",
      "dev batch 26/33\n",
      "dev batch 27/33\n",
      "dev batch 28/33\n",
      "dev batch 29/33\n",
      "dev batch 30/33\n",
      "dev batch 31/33\n",
      "dev batch 32/33\n",
      "dev batch 33/33\n",
      "\n",
      "BERTweet_embeddings_dev: torch.Size([271, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_dev.pickle\n",
      "Split: test\n",
      "Num samples: 548\n",
      "Num batches: 68\n",
      "test batch 0/68\n",
      "test batch 1/68\n",
      "test batch 2/68\n",
      "test batch 3/68\n",
      "test batch 4/68\n",
      "test batch 5/68\n",
      "test batch 6/68\n",
      "test batch 7/68\n",
      "test batch 8/68\n",
      "test batch 9/68\n",
      "test batch 10/68\n",
      "test batch 11/68\n",
      "test batch 12/68\n",
      "test batch 13/68\n",
      "test batch 14/68\n",
      "test batch 15/68\n",
      "test batch 16/68\n",
      "test batch 17/68\n",
      "test batch 18/68\n",
      "test batch 19/68\n",
      "test batch 20/68\n",
      "test batch 21/68\n",
      "test batch 22/68\n",
      "test batch 23/68\n",
      "test batch 24/68\n",
      "test batch 25/68\n",
      "test batch 26/68\n",
      "test batch 27/68\n",
      "test batch 28/68\n",
      "test batch 29/68\n",
      "test batch 30/68\n",
      "test batch 31/68\n",
      "test batch 32/68\n",
      "test batch 33/68\n",
      "test batch 34/68\n",
      "test batch 35/68\n",
      "test batch 36/68\n",
      "test batch 37/68\n",
      "test batch 38/68\n",
      "test batch 39/68\n",
      "test batch 40/68\n",
      "test batch 41/68\n",
      "test batch 42/68\n",
      "test batch 43/68\n",
      "test batch 44/68\n",
      "test batch 45/68\n",
      "test batch 46/68\n",
      "test batch 47/68\n",
      "test batch 48/68\n",
      "test batch 49/68\n",
      "test batch 50/68\n",
      "test batch 51/68\n",
      "test batch 52/68\n",
      "test batch 53/68\n",
      "test batch 54/68\n",
      "test batch 55/68\n",
      "test batch 56/68\n",
      "test batch 57/68\n",
      "test batch 58/68\n",
      "test batch 59/68\n",
      "test batch 60/68\n",
      "test batch 61/68\n",
      "test batch 62/68\n",
      "test batch 63/68\n",
      "test batch 64/68\n",
      "test batch 65/68\n",
      "test batch 66/68\n",
      "test batch 67/68\n",
      "test batch 68/68\n",
      "\n",
      "BERTweet_embeddings_test: torch.Size([548, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_test.pickle\n",
      "Split: gold\n",
      "Num samples: 736\n",
      "Num batches: 92\n",
      "gold batch 0/92\n",
      "gold batch 1/92\n",
      "gold batch 2/92\n",
      "gold batch 3/92\n",
      "gold batch 4/92\n",
      "gold batch 5/92\n",
      "gold batch 6/92\n",
      "gold batch 7/92\n",
      "gold batch 8/92\n",
      "gold batch 9/92\n",
      "gold batch 10/92\n",
      "gold batch 11/92\n",
      "gold batch 12/92\n",
      "gold batch 13/92\n",
      "gold batch 14/92\n",
      "gold batch 15/92\n",
      "gold batch 16/92\n",
      "gold batch 17/92\n",
      "gold batch 18/92\n",
      "gold batch 19/92\n",
      "gold batch 20/92\n",
      "gold batch 21/92\n",
      "gold batch 22/92\n",
      "gold batch 23/92\n",
      "gold batch 24/92\n",
      "gold batch 25/92\n",
      "gold batch 26/92\n",
      "gold batch 27/92\n",
      "gold batch 28/92\n",
      "gold batch 29/92\n",
      "gold batch 30/92\n",
      "gold batch 31/92\n",
      "gold batch 32/92\n",
      "gold batch 33/92\n",
      "gold batch 34/92\n",
      "gold batch 35/92\n",
      "gold batch 36/92\n",
      "gold batch 37/92\n",
      "gold batch 38/92\n",
      "gold batch 39/92\n",
      "gold batch 40/92\n",
      "gold batch 41/92\n",
      "gold batch 42/92\n",
      "gold batch 43/92\n",
      "gold batch 44/92\n",
      "gold batch 45/92\n",
      "gold batch 46/92\n",
      "gold batch 47/92\n",
      "gold batch 48/92\n",
      "gold batch 49/92\n",
      "gold batch 50/92\n",
      "gold batch 51/92\n",
      "gold batch 52/92\n",
      "gold batch 53/92\n",
      "gold batch 54/92\n",
      "gold batch 55/92\n",
      "gold batch 56/92\n",
      "gold batch 57/92\n",
      "gold batch 58/92\n",
      "gold batch 59/92\n",
      "gold batch 60/92\n",
      "gold batch 61/92\n",
      "gold batch 62/92\n",
      "gold batch 63/92\n",
      "gold batch 64/92\n",
      "gold batch 65/92\n",
      "gold batch 66/92\n",
      "gold batch 67/92\n",
      "gold batch 68/92\n",
      "gold batch 69/92\n",
      "gold batch 70/92\n",
      "gold batch 71/92\n",
      "gold batch 72/92\n",
      "gold batch 73/92\n",
      "gold batch 74/92\n",
      "gold batch 75/92\n",
      "gold batch 76/92\n",
      "gold batch 77/92\n",
      "gold batch 78/92\n",
      "gold batch 79/92\n",
      "gold batch 80/92\n",
      "gold batch 81/92\n",
      "gold batch 82/92\n",
      "gold batch 83/92\n",
      "gold batch 84/92\n",
      "gold batch 85/92\n",
      "gold batch 86/92\n",
      "gold batch 87/92\n",
      "gold batch 88/92\n",
      "gold batch 89/92\n",
      "gold batch 90/92\n",
      "gold batch 91/92\n",
      "\n",
      "BERTweet_embeddings_gold: torch.Size([736, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_gold.pickle\n"
     ]
    }
   ],
   "source": [
    "# Embed every split\n",
    "for split in SPLITS:\n",
    "    utils.embed_and_pickle_split_with_bertweet(bertweet, data_dir_with_version, split, tweets_encoded[split], with_ocr=False, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train\n",
      "Num samples: 2356\n",
      "Num batches: 294\n",
      "train batch 0/294\n",
      "train batch 1/294\n",
      "train batch 2/294\n",
      "train batch 3/294\n",
      "train batch 4/294\n",
      "train batch 5/294\n",
      "train batch 6/294\n",
      "train batch 7/294\n",
      "train batch 8/294\n",
      "train batch 9/294\n",
      "train batch 10/294\n",
      "train batch 11/294\n",
      "train batch 12/294\n",
      "train batch 13/294\n",
      "train batch 14/294\n",
      "train batch 15/294\n",
      "train batch 16/294\n",
      "train batch 17/294\n",
      "train batch 18/294\n",
      "train batch 19/294\n",
      "train batch 20/294\n",
      "train batch 21/294\n",
      "train batch 22/294\n",
      "train batch 23/294\n",
      "train batch 24/294\n",
      "train batch 25/294\n",
      "train batch 26/294\n",
      "train batch 27/294\n",
      "train batch 28/294\n",
      "train batch 29/294\n",
      "train batch 30/294\n",
      "train batch 31/294\n",
      "train batch 32/294\n",
      "train batch 33/294\n",
      "train batch 34/294\n",
      "train batch 35/294\n",
      "train batch 36/294\n",
      "train batch 37/294\n",
      "train batch 38/294\n",
      "train batch 39/294\n",
      "train batch 40/294\n",
      "train batch 41/294\n",
      "train batch 42/294\n",
      "train batch 43/294\n",
      "train batch 44/294\n",
      "train batch 45/294\n",
      "train batch 46/294\n",
      "train batch 47/294\n",
      "train batch 48/294\n",
      "train batch 49/294\n",
      "train batch 50/294\n",
      "train batch 51/294\n",
      "train batch 52/294\n",
      "train batch 53/294\n",
      "train batch 54/294\n",
      "train batch 55/294\n",
      "train batch 56/294\n",
      "train batch 57/294\n",
      "train batch 58/294\n",
      "train batch 59/294\n",
      "train batch 60/294\n",
      "train batch 61/294\n",
      "train batch 62/294\n",
      "train batch 63/294\n",
      "train batch 64/294\n",
      "train batch 65/294\n",
      "train batch 66/294\n",
      "train batch 67/294\n",
      "train batch 68/294\n",
      "train batch 69/294\n",
      "train batch 70/294\n",
      "train batch 71/294\n",
      "train batch 72/294\n",
      "train batch 73/294\n",
      "train batch 74/294\n",
      "train batch 75/294\n",
      "train batch 76/294\n",
      "train batch 77/294\n",
      "train batch 78/294\n",
      "train batch 79/294\n",
      "train batch 80/294\n",
      "train batch 81/294\n",
      "train batch 82/294\n",
      "train batch 83/294\n",
      "train batch 84/294\n",
      "train batch 85/294\n",
      "train batch 86/294\n",
      "train batch 87/294\n",
      "train batch 88/294\n",
      "train batch 89/294\n",
      "train batch 90/294\n",
      "train batch 91/294\n",
      "train batch 92/294\n",
      "train batch 93/294\n",
      "train batch 94/294\n",
      "train batch 95/294\n",
      "train batch 96/294\n",
      "train batch 97/294\n",
      "train batch 98/294\n",
      "train batch 99/294\n",
      "train batch 100/294\n",
      "train batch 101/294\n",
      "train batch 102/294\n",
      "train batch 103/294\n",
      "train batch 104/294\n",
      "train batch 105/294\n",
      "train batch 106/294\n",
      "train batch 107/294\n",
      "train batch 108/294\n",
      "train batch 109/294\n",
      "train batch 110/294\n",
      "train batch 111/294\n",
      "train batch 112/294\n",
      "train batch 113/294\n",
      "train batch 114/294\n",
      "train batch 115/294\n",
      "train batch 116/294\n",
      "train batch 117/294\n",
      "train batch 118/294\n",
      "train batch 119/294\n",
      "train batch 120/294\n",
      "train batch 121/294\n",
      "train batch 122/294\n",
      "train batch 123/294\n",
      "train batch 124/294\n",
      "train batch 125/294\n",
      "train batch 126/294\n",
      "train batch 127/294\n",
      "train batch 128/294\n",
      "train batch 129/294\n",
      "train batch 130/294\n",
      "train batch 131/294\n",
      "train batch 132/294\n",
      "train batch 133/294\n",
      "train batch 134/294\n",
      "train batch 135/294\n",
      "train batch 136/294\n",
      "train batch 137/294\n",
      "train batch 138/294\n",
      "train batch 139/294\n",
      "train batch 140/294\n",
      "train batch 141/294\n",
      "train batch 142/294\n",
      "train batch 143/294\n",
      "train batch 144/294\n",
      "train batch 145/294\n",
      "train batch 146/294\n",
      "train batch 147/294\n",
      "train batch 148/294\n",
      "train batch 149/294\n",
      "train batch 150/294\n",
      "train batch 151/294\n",
      "train batch 152/294\n",
      "train batch 153/294\n",
      "train batch 154/294\n",
      "train batch 155/294\n",
      "train batch 156/294\n",
      "train batch 157/294\n",
      "train batch 158/294\n",
      "train batch 159/294\n",
      "train batch 160/294\n",
      "train batch 161/294\n",
      "train batch 162/294\n",
      "train batch 163/294\n",
      "train batch 164/294\n",
      "train batch 165/294\n",
      "train batch 166/294\n",
      "train batch 167/294\n",
      "train batch 168/294\n",
      "train batch 169/294\n",
      "train batch 170/294\n",
      "train batch 171/294\n",
      "train batch 172/294\n",
      "train batch 173/294\n",
      "train batch 174/294\n",
      "train batch 175/294\n",
      "train batch 176/294\n",
      "train batch 177/294\n",
      "train batch 178/294\n",
      "train batch 179/294\n",
      "train batch 180/294\n",
      "train batch 181/294\n",
      "train batch 182/294\n",
      "train batch 183/294\n",
      "train batch 184/294\n",
      "train batch 185/294\n",
      "train batch 186/294\n",
      "train batch 187/294\n",
      "train batch 188/294\n",
      "train batch 189/294\n",
      "train batch 190/294\n",
      "train batch 191/294\n",
      "train batch 192/294\n",
      "train batch 193/294\n",
      "train batch 194/294\n",
      "train batch 195/294\n",
      "train batch 196/294\n",
      "train batch 197/294\n",
      "train batch 198/294\n",
      "train batch 199/294\n",
      "train batch 200/294\n",
      "train batch 201/294\n",
      "train batch 202/294\n",
      "train batch 203/294\n",
      "train batch 204/294\n",
      "train batch 205/294\n",
      "train batch 206/294\n",
      "train batch 207/294\n",
      "train batch 208/294\n",
      "train batch 209/294\n",
      "train batch 210/294\n",
      "train batch 211/294\n",
      "train batch 212/294\n",
      "train batch 213/294\n",
      "train batch 214/294\n",
      "train batch 215/294\n",
      "train batch 216/294\n",
      "train batch 217/294\n",
      "train batch 218/294\n",
      "train batch 219/294\n",
      "train batch 220/294\n",
      "train batch 221/294\n",
      "train batch 222/294\n",
      "train batch 223/294\n",
      "train batch 224/294\n",
      "train batch 225/294\n",
      "train batch 226/294\n",
      "train batch 227/294\n",
      "train batch 228/294\n",
      "train batch 229/294\n",
      "train batch 230/294\n",
      "train batch 231/294\n",
      "train batch 232/294\n",
      "train batch 233/294\n",
      "train batch 234/294\n",
      "train batch 235/294\n",
      "train batch 236/294\n",
      "train batch 237/294\n",
      "train batch 238/294\n",
      "train batch 239/294\n",
      "train batch 240/294\n",
      "train batch 241/294\n",
      "train batch 242/294\n",
      "train batch 243/294\n",
      "train batch 244/294\n",
      "train batch 245/294\n",
      "train batch 246/294\n",
      "train batch 247/294\n",
      "train batch 248/294\n",
      "train batch 249/294\n",
      "train batch 250/294\n",
      "train batch 251/294\n",
      "train batch 252/294\n",
      "train batch 253/294\n",
      "train batch 254/294\n",
      "train batch 255/294\n",
      "train batch 256/294\n",
      "train batch 257/294\n",
      "train batch 258/294\n",
      "train batch 259/294\n",
      "train batch 260/294\n",
      "train batch 261/294\n",
      "train batch 262/294\n",
      "train batch 263/294\n",
      "train batch 264/294\n",
      "train batch 265/294\n",
      "train batch 266/294\n",
      "train batch 267/294\n",
      "train batch 268/294\n",
      "train batch 269/294\n",
      "train batch 270/294\n",
      "train batch 271/294\n",
      "train batch 272/294\n",
      "train batch 273/294\n",
      "train batch 274/294\n",
      "train batch 275/294\n",
      "train batch 276/294\n",
      "train batch 277/294\n",
      "train batch 278/294\n",
      "train batch 279/294\n",
      "train batch 280/294\n",
      "train batch 281/294\n",
      "train batch 282/294\n",
      "train batch 283/294\n",
      "train batch 284/294\n",
      "train batch 285/294\n",
      "train batch 286/294\n",
      "train batch 287/294\n",
      "train batch 288/294\n",
      "train batch 289/294\n",
      "train batch 290/294\n",
      "train batch 291/294\n",
      "train batch 292/294\n",
      "train batch 293/294\n",
      "train batch 294/294\n",
      "\n",
      "BERTweet_embeddings_with_ocr_train: torch.Size([2356, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_with_ocr_train.pickle\n",
      "Split: dev\n",
      "Num samples: 271\n",
      "Num batches: 33\n",
      "dev batch 0/33\n",
      "dev batch 1/33\n",
      "dev batch 2/33\n",
      "dev batch 3/33\n",
      "dev batch 4/33\n",
      "dev batch 5/33\n",
      "dev batch 6/33\n",
      "dev batch 7/33\n",
      "dev batch 8/33\n",
      "dev batch 9/33\n",
      "dev batch 10/33\n",
      "dev batch 11/33\n",
      "dev batch 12/33\n",
      "dev batch 13/33\n",
      "dev batch 14/33\n",
      "dev batch 15/33\n",
      "dev batch 16/33\n",
      "dev batch 17/33\n",
      "dev batch 18/33\n",
      "dev batch 19/33\n",
      "dev batch 20/33\n",
      "dev batch 21/33\n",
      "dev batch 22/33\n",
      "dev batch 23/33\n",
      "dev batch 24/33\n",
      "dev batch 25/33\n",
      "dev batch 26/33\n",
      "dev batch 27/33\n",
      "dev batch 28/33\n",
      "dev batch 29/33\n",
      "dev batch 30/33\n",
      "dev batch 31/33\n",
      "dev batch 32/33\n",
      "dev batch 33/33\n",
      "\n",
      "BERTweet_embeddings_with_ocr_dev: torch.Size([271, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_with_ocr_dev.pickle\n",
      "Split: test\n",
      "Num samples: 548\n",
      "Num batches: 68\n",
      "test batch 0/68\n",
      "test batch 1/68\n",
      "test batch 2/68\n",
      "test batch 3/68\n",
      "test batch 4/68\n",
      "test batch 5/68\n",
      "test batch 6/68\n",
      "test batch 7/68\n",
      "test batch 8/68\n",
      "test batch 9/68\n",
      "test batch 10/68\n",
      "test batch 11/68\n",
      "test batch 12/68\n",
      "test batch 13/68\n",
      "test batch 14/68\n",
      "test batch 15/68\n",
      "test batch 16/68\n",
      "test batch 17/68\n",
      "test batch 18/68\n",
      "test batch 19/68\n",
      "test batch 20/68\n",
      "test batch 21/68\n",
      "test batch 22/68\n",
      "test batch 23/68\n",
      "test batch 24/68\n",
      "test batch 25/68\n",
      "test batch 26/68\n",
      "test batch 27/68\n",
      "test batch 28/68\n",
      "test batch 29/68\n",
      "test batch 30/68\n",
      "test batch 31/68\n",
      "test batch 32/68\n",
      "test batch 33/68\n",
      "test batch 34/68\n",
      "test batch 35/68\n",
      "test batch 36/68\n",
      "test batch 37/68\n",
      "test batch 38/68\n",
      "test batch 39/68\n",
      "test batch 40/68\n",
      "test batch 41/68\n",
      "test batch 42/68\n",
      "test batch 43/68\n",
      "test batch 44/68\n",
      "test batch 45/68\n",
      "test batch 46/68\n",
      "test batch 47/68\n",
      "test batch 48/68\n",
      "test batch 49/68\n",
      "test batch 50/68\n",
      "test batch 51/68\n",
      "test batch 52/68\n",
      "test batch 53/68\n",
      "test batch 54/68\n",
      "test batch 55/68\n",
      "test batch 56/68\n",
      "test batch 57/68\n",
      "test batch 58/68\n",
      "test batch 59/68\n",
      "test batch 60/68\n",
      "test batch 61/68\n",
      "test batch 62/68\n",
      "test batch 63/68\n",
      "test batch 64/68\n",
      "test batch 65/68\n",
      "test batch 66/68\n",
      "test batch 67/68\n",
      "test batch 68/68\n",
      "\n",
      "BERTweet_embeddings_with_ocr_test: torch.Size([548, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_with_ocr_test.pickle\n",
      "Split: gold\n",
      "Num samples: 736\n",
      "Num batches: 92\n",
      "gold batch 0/92\n",
      "gold batch 1/92\n",
      "gold batch 2/92\n",
      "gold batch 3/92\n",
      "gold batch 4/92\n",
      "gold batch 5/92\n",
      "gold batch 6/92\n",
      "gold batch 7/92\n",
      "gold batch 8/92\n",
      "gold batch 9/92\n",
      "gold batch 10/92\n",
      "gold batch 11/92\n",
      "gold batch 12/92\n",
      "gold batch 13/92\n",
      "gold batch 14/92\n",
      "gold batch 15/92\n",
      "gold batch 16/92\n",
      "gold batch 17/92\n",
      "gold batch 18/92\n",
      "gold batch 19/92\n",
      "gold batch 20/92\n",
      "gold batch 21/92\n",
      "gold batch 22/92\n",
      "gold batch 23/92\n",
      "gold batch 24/92\n",
      "gold batch 25/92\n",
      "gold batch 26/92\n",
      "gold batch 27/92\n",
      "gold batch 28/92\n",
      "gold batch 29/92\n",
      "gold batch 30/92\n",
      "gold batch 31/92\n",
      "gold batch 32/92\n",
      "gold batch 33/92\n",
      "gold batch 34/92\n",
      "gold batch 35/92\n",
      "gold batch 36/92\n",
      "gold batch 37/92\n",
      "gold batch 38/92\n",
      "gold batch 39/92\n",
      "gold batch 40/92\n",
      "gold batch 41/92\n",
      "gold batch 42/92\n",
      "gold batch 43/92\n",
      "gold batch 44/92\n",
      "gold batch 45/92\n",
      "gold batch 46/92\n",
      "gold batch 47/92\n",
      "gold batch 48/92\n",
      "gold batch 49/92\n",
      "gold batch 50/92\n",
      "gold batch 51/92\n",
      "gold batch 52/92\n",
      "gold batch 53/92\n",
      "gold batch 54/92\n",
      "gold batch 55/92\n",
      "gold batch 56/92\n",
      "gold batch 57/92\n",
      "gold batch 58/92\n",
      "gold batch 59/92\n",
      "gold batch 60/92\n",
      "gold batch 61/92\n",
      "gold batch 62/92\n",
      "gold batch 63/92\n",
      "gold batch 64/92\n",
      "gold batch 65/92\n",
      "gold batch 66/92\n",
      "gold batch 67/92\n",
      "gold batch 68/92\n",
      "gold batch 69/92\n",
      "gold batch 70/92\n",
      "gold batch 71/92\n",
      "gold batch 72/92\n",
      "gold batch 73/92\n",
      "gold batch 74/92\n",
      "gold batch 75/92\n",
      "gold batch 76/92\n",
      "gold batch 77/92\n",
      "gold batch 78/92\n",
      "gold batch 79/92\n",
      "gold batch 80/92\n",
      "gold batch 81/92\n",
      "gold batch 82/92\n",
      "gold batch 83/92\n",
      "gold batch 84/92\n",
      "gold batch 85/92\n",
      "gold batch 86/92\n",
      "gold batch 87/92\n",
      "gold batch 88/92\n",
      "gold batch 89/92\n",
      "gold batch 90/92\n",
      "gold batch 91/92\n",
      "\n",
      "BERTweet_embeddings_with_ocr_gold: torch.Size([736, 768])\n",
      "Pickled the embeddings: /home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_v2/BERTweet_embeddings_with_ocr_gold.pickle\n"
     ]
    }
   ],
   "source": [
    "# Embed every split\n",
    "for split in SPLITS:\n",
    "    utils.embed_and_pickle_split_with_bertweet(bertweet, data_dir_with_version, split, concat_encoded[split], with_ocr=True, batch_size=batch_size, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Load the Pickled Embeddings\n",
    "Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([736, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load embedding tensor of one data split from pickle file\n",
    "pickle_file = f\"{data_dir_with_version}/BERTweet_embeddings_with_ocr_{GOLD}.pickle\"\n",
    "with open(pickle_file, 'rb') as handle:\n",
    "    embeddings_tensor = pickle.load(handle)\n",
    "    print(embeddings_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw1_new_kernel",
   "language": "python",
   "name": "cw1_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
