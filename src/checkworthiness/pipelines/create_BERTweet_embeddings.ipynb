{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Notebook Summary\n",
    "- Get BERTweet from huggingface\n",
    "- Use BERTweet to\n",
    "    - extract embeddings from Tweets\n",
    "    - extract embeddings from concatenations of Tweet + OCR text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0. Imports and Constants\n",
    "- Do not forget to select dataset version in the #CONSTANTS# part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T18:07:32.321626748Z",
     "start_time": "2023-07-09T18:07:32.253071738Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "############## AUTORELOAD MAGIC ###################\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "###################################################\n",
    "\n",
    "############## FUNDAMENTAL MODULES ################\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "##################################################\n",
    "\n",
    "############## TASK-SPECIFIC MODULES #############\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from data import TweetNormalizer, utils, feature_extraction\n",
    "###################################################\n",
    "\n",
    "############## DATA SCIENCE & ML MODULES ##########\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy import stats\n",
    "###################################################\n",
    "\n",
    "############# CONSTANT DICT KEYS ###################\n",
    "# Constant dict keys\n",
    "TRAIN = \"train\"\n",
    "DEV = \"dev\"\n",
    "TEST = \"test\"\n",
    "GOLD = \"gold\"\n",
    "TXT = \"txt\"\n",
    "IMG = \"img\"\n",
    "OCR = \"ocr\"\n",
    "TXT_OCR = \"txt_ocr\"\n",
    "SPLITS = [TRAIN, DEV, TEST, GOLD]\n",
    "####################################################\n",
    "\n",
    "####################### SELECT ###########################\n",
    "users = [\"patriziopalmisano\", \"onurdenizguler\", \"jockl\"]\n",
    "user = users[2] # SELECT USER\n",
    "version = \"v2\" # SELECT DATASET VERSION\n",
    "dataset_version = version\n",
    "##########################################################\n",
    "\n",
    "if user in users[:2]:\n",
    "    cw_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive\"\n",
    "    data_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english\"\n",
    "    data_dir_with_version = f\"{data_dir}_{dataset_version}\"\n",
    "    gold_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english_test_gold\"\n",
    "\n",
    "else:\n",
    "    cw_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive\"\n",
    "    data_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english\"\n",
    "    data_dir_with_version = f\"{data_dir}_{dataset_version}\"\n",
    "    gold_dir = f\"{cw_dir}/data/CT23_1A_checkworthy_multimodal_english_test_gold\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Load all Datasets\n",
    "First, we extract all the raw texts from the JSON files. Note that the concatenation of tweet and OCR text is realized with a new line in-between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of txt, img, ocr, txt+ocr arrays in train, test, dev, gold:\n",
      "2356 2356 2356 2356\n",
      "271 271 2356 2356\n",
      "548 548 2356 2356\n",
      "736 736 2356 2356\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "raw_dataset, tweet_texts, imgs, tweet_ids, ocr_texts, tweet_concat_ocr = utils.load_data_splits_with_gold_dataset(data_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "\n",
      "OCR:\n",
      "CORONAVIRUS\n",
      "MGN\n",
      "\n",
      "\n",
      "Concat:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "CORONAVIRUS\n",
      "MGN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect Tweet and OCR concatenation\n",
    "print(f\"Tweet:\\n{tweet_texts[TRAIN][6]}\")\n",
    "print(f\"\\nOCR:\\n{ocr_texts[TRAIN][6]}\")\n",
    "print(f\"\\nConcat:\\n{tweet_concat_ocr[TRAIN][6]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Normalize Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n",
      "2356\n"
     ]
    }
   ],
   "source": [
    "# Normalize all tweets using TweetNormalizer()\n",
    "normalized_tweets = {split: [TweetNormalizer.normalizeTweet(tweet) for tweet in tweet_texts[split]] for split in SPLITS}\n",
    "normalized_tweet_concat_ocr = {split: [TweetNormalizer.normalizeTweet(concat) for concat in tweet_concat_ocr[split]] for split in SPLITS}\n",
    "print(len(normalized_tweets[TRAIN]))\n",
    "print(len(normalized_tweet_concat_ocr[TRAIN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "\n",
      "Normalized Tweet:\n",
      "Despite calls for calm , some local people are panicking over the deadly coronavirus outbreak . \" I really am . It 's really something that I 'm really frightened about right now . \" \" It might be actually worse than what they 're telling us . \" HTTPURL HTTPURL\n"
     ]
    }
   ],
   "source": [
    "# Inspect normalization of tweets\n",
    "print(f\"Tweet:\\n{tweet_texts[TRAIN][6]}\")\n",
    "print(f\"\\nNormalized Tweet:\\n{normalized_tweets[TRAIN][6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat:\n",
      "Despite calls for calm, some local people are panicking over the deadly coronavirus outbreak.\n",
      "\n",
      "\"I really am. It's really something that I'm really frightened about right now.\"\n",
      "\n",
      "\"It might be actually worse than what they're telling us.\"\n",
      "\n",
      "https://t.co/2aeFCPdg4T https://t.co/Stjz0XlcwA\n",
      "Normalized Concat:\n",
      "Despite calls for calm , some local people are panicking over the deadly coronavirus outbreak . \" I really am . It 's really something that I 'm really frightened about right now . \" \" It might be actually worse than what they 're telling us . \" HTTPURL HTTPURL CORONAVIRUS MGN\n"
     ]
    }
   ],
   "source": [
    "# Inspect normalization of tweet_ocr_concat\n",
    "print(f\"Concat:\\n{tweet_texts[TRAIN][6]}\")\n",
    "print(f\"Normalized Concat:\\n{normalized_tweet_concat_ocr[TRAIN][6]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set Up BERTweet and Embed Minimal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago Cardinal : Global Warming , Migrants Are ‘ Bigger Agenda ' than Sex Abuse HTTPURL via @USER HTTPURL\n",
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "# Encode one example text\n",
    "input_text = normalized_tweets[TRAIN][1]\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text)])\n",
    "print(input_text)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Embed the example\n",
    "with torch.no_grad():\n",
    "    features = bertweet(input_ids)\n",
    "print(features.pooler_output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resolve the Sequence Length Issue\n",
    "Maximum number of tokens for BERTweet input: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token limit and padding ID\n",
    "token_limit = 128\n",
    "padding_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# First, tokenize everything\n",
    "normalized_tweets_tokenized = {split: tokenizer(normalized_tweets[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}\n",
    "normalized_concat_tokenized = {split: tokenizer(normalized_tweet_concat_ocr[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the blunt attempt of tokenizing yields the warning that BERTweet only accepts input sequences of length < 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([116])\n",
      "torch.Size([3586])\n"
     ]
    }
   ],
   "source": [
    "# Inspect tokenized sample\n",
    "print(normalized_tweets_tokenized[TRAIN][0].shape)\n",
    "print(normalized_concat_tokenized[TRAIN][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the plain tweets never exceed the token limit, at least one sample from the tweet+OCR concatenations conatains 3586 tokens. This results in padding any sequence to this number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ID, Length, Excess Tokens)\n",
      "Tweets: 0\t[]\n",
      "Concat: 296\t[(651, 129, 1), (1144, 129, 1), (1985, 129, 1), (526, 130, 2), (1410, 130, 2), (1454, 130, 2), (1738, 130, 2), (1338, 131, 3), (1978, 131, 3), (769, 132, 4), (1615, 132, 4), (67, 133, 5), (471, 133, 5), (1070, 133, 5), (1136, 133, 5), (1519, 133, 5), (1984, 133, 5), (1991, 133, 5), (330, 134, 6), (367, 134, 6), (1715, 134, 6), (2007, 134, 6), (120, 135, 7), (2045, 135, 7), (2205, 135, 7), (318, 136, 8), (672, 136, 8), (137, 137, 9), (1770, 137, 9), (279, 138, 10), (1053, 138, 10), (1429, 138, 10), (59, 139, 11), (762, 139, 11), (1902, 139, 11), (830, 140, 12), (2130, 140, 12), (1097, 141, 13), (1821, 141, 13), (785, 142, 14), (1700, 142, 14), (2290, 142, 14), (215, 143, 15), (1408, 143, 15), (1710, 144, 16), (2006, 144, 16), (631, 148, 20), (1425, 149, 21), (1956, 149, 21), (2138, 149, 21), (109, 150, 22), (1026, 150, 22), (1780, 150, 22), (371, 151, 23), (862, 151, 23), (2016, 151, 23), (2222, 151, 23), (657, 152, 24), (51, 153, 25), (385, 153, 25), (704, 153, 25), (766, 153, 25), (1448, 153, 25), (1899, 153, 25), (1039, 155, 27), (2325, 155, 27), (174, 157, 29), (715, 157, 29), (747, 157, 29), (1388, 157, 29), (89, 159, 31), (347, 159, 31), (713, 159, 31), (1651, 159, 31), (1714, 159, 31), (772, 160, 32), (1895, 162, 34), (2157, 162, 34), (490, 163, 35), (13, 164, 36), (681, 164, 36), (1221, 164, 36), (2046, 164, 36), (2165, 164, 36), (0, 165, 37), (231, 165, 37), (1276, 165, 37), (2288, 166, 38), (1854, 167, 39), (2256, 167, 39), (453, 168, 40), (537, 169, 41), (55, 170, 42), (2224, 170, 42), (462, 171, 43), (635, 172, 44), (1758, 172, 44), (1509, 173, 45), (225, 174, 46), (767, 174, 46), (1261, 174, 46), (1302, 175, 47), (472, 177, 49), (606, 177, 49), (1379, 177, 49), (300, 178, 50), (358, 178, 50), (1248, 178, 50), (1496, 178, 50), (321, 179, 51), (1778, 179, 51), (2354, 179, 51), (236, 180, 52), (1278, 180, 52), (2177, 180, 52), (1151, 181, 53), (1427, 181, 53), (150, 182, 54), (2076, 182, 54), (987, 183, 55), (2301, 183, 55), (1824, 184, 56), (2061, 184, 56), (1212, 185, 57), (1385, 185, 57), (1350, 186, 58), (690, 187, 59), (1659, 187, 59), (1771, 187, 59), (2079, 187, 59), (167, 188, 60), (1655, 188, 60), (1663, 188, 60), (764, 189, 61), (1352, 189, 61), (1523, 190, 62), (1560, 191, 63), (1860, 191, 63), (1981, 192, 64), (708, 193, 65), (1474, 193, 65), (2204, 193, 65), (476, 194, 66), (143, 195, 67), (1317, 196, 68), (1106, 198, 70), (2114, 199, 71), (295, 200, 72), (811, 200, 72), (1995, 201, 73), (2355, 201, 73), (675, 203, 75), (1092, 203, 75), (2044, 203, 75), (365, 204, 76), (1399, 204, 76), (679, 205, 77), (1722, 205, 77), (2162, 205, 77), (2282, 205, 77), (863, 206, 78), (953, 210, 82), (1912, 213, 85), (204, 214, 86), (76, 215, 87), (976, 215, 87), (2324, 215, 87), (784, 216, 88), (820, 218, 90), (1348, 218, 90), (15, 219, 91), (661, 219, 91), (1022, 221, 93), (1888, 222, 94), (2160, 223, 95), (827, 224, 96), (1467, 224, 96), (1090, 225, 97), (163, 228, 100), (1198, 228, 100), (1840, 228, 100), (1717, 234, 106), (2317, 235, 107), (735, 236, 108), (1314, 236, 108), (1593, 236, 108), (1142, 237, 109), (1660, 238, 110), (1456, 240, 112), (1936, 240, 112), (2233, 241, 113), (659, 243, 115), (1801, 248, 120), (2209, 248, 120), (1863, 251, 123), (1734, 253, 125), (1346, 256, 128), (1977, 256, 128), (257, 258, 130), (356, 259, 131), (2161, 259, 131), (2262, 259, 131), (1190, 260, 132), (1326, 262, 134), (406, 268, 140), (1883, 272, 144), (1368, 274, 146), (776, 277, 149), (505, 278, 150), (79, 279, 151), (1363, 280, 152), (1884, 280, 152), (1749, 281, 153), (34, 282, 154), (1445, 283, 155), (296, 285, 157), (1239, 288, 160), (1943, 288, 160), (463, 292, 164), (709, 296, 168), (9, 297, 169), (415, 297, 169), (155, 298, 170), (1958, 300, 172), (1587, 303, 175), (967, 304, 176), (1024, 304, 176), (408, 306, 178), (1224, 307, 179), (1699, 309, 181), (1312, 311, 183), (826, 313, 185), (165, 316, 188), (2219, 316, 188), (600, 318, 190), (1761, 318, 190), (945, 319, 191), (260, 322, 194), (1229, 332, 204), (731, 334, 206), (2066, 335, 207), (2051, 338, 210), (955, 340, 212), (2146, 346, 218), (528, 349, 221), (625, 349, 221), (8, 350, 222), (2126, 350, 222), (1184, 351, 223), (1736, 353, 225), (1799, 356, 228), (1113, 357, 229), (1727, 372, 244), (27, 379, 251), (382, 383, 255), (1325, 393, 265), (2307, 393, 265), (2239, 395, 267), (823, 399, 271), (1127, 399, 271), (1604, 416, 288), (1543, 419, 291), (1624, 419, 291), (749, 422, 294), (2017, 436, 308), (130, 445, 317), (744, 447, 319), (2133, 447, 319), (753, 452, 324), (918, 465, 337), (2021, 471, 343), (1518, 485, 357), (618, 503, 375), (1025, 517, 389), (861, 519, 391), (1139, 528, 400), (1105, 545, 417), (2186, 563, 435), (574, 573, 445), (2057, 581, 453), (1622, 584, 456), (737, 589, 461), (1461, 617, 489), (2302, 679, 551), (1079, 682, 554), (1403, 702, 574), (86, 704, 576), (502, 751, 623), (2291, 849, 721), (2008, 907, 779), (5, 908, 780), (556, 1051, 923), (1594, 1250, 1122), (2321, 1275, 1147), (1628, 1620, 1492), (1698, 3586, 3458)]\n"
     ]
    }
   ],
   "source": [
    "# Find all the examples with excess tokens\n",
    "split_to_tweets_with_excess_tokens = utils.get_samples_with_excess_tokens(token_limit, normalized_tweets_tokenized, padding_idx)\n",
    "split_to_concat_with_excess_tokens = utils.get_samples_with_excess_tokens(token_limit, normalized_concat_tokenized, padding_idx)\n",
    "print(f\"(ID, Length, Excess Tokens)\")\n",
    "print(f\"Tweets: {len(split_to_tweets_with_excess_tokens[TRAIN])}\\t{split_to_tweets_with_excess_tokens[TRAIN]}\")\n",
    "print(f\"Concat: {len(split_to_concat_with_excess_tokens[TRAIN])}\\t{split_to_concat_with_excess_tokens[TRAIN]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst case: We truncate 296 examples (for the train set) from the tweet+OCR concatenations. Not a lot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now further normalize the samples that features excess tokens. If this further normalizations still yields a sequence that is too long, the function automatically truncates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further normalize/truncate those examples with excess tokens\n",
    "final_normalized_tweets = utils.further_normalize_samples_with_excess_tokens(token_limit, normalized_tweets, split_to_tweets_with_excess_tokens, normalized_tweets_tokenized, tokenizer, padding_idx)\n",
    "final_normalized_concat = utils.further_normalize_samples_with_excess_tokens(token_limit, normalized_tweet_concat_ocr, split_to_concat_with_excess_tokens, normalized_concat_tokenized, tokenizer, padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize final texts\n",
    "tweets_tokenized = {split: tokenizer(final_normalized_tweets[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}\n",
    "concat_tokenized = {split: tokenizer(final_normalized_concat[split], padding=True, return_tensors=\"pt\")[\"input_ids\"] for split in SPLITS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: torch.Size([116])\ttorch.Size([116])\n",
      "Concat: torch.Size([3586])\ttorch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Inspect tokenized example before/after normalization and truncation\n",
    "print(f\"Tweet: {normalized_tweets_tokenized[TRAIN][0].shape}\\t{tweets_tokenized[TRAIN][0].shape}\")\n",
    "print(f\"Concat: {normalized_concat_tokenized[TRAIN][0].shape}\\t{concat_tokenized[TRAIN][0].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now, the concat sequences are padded only up to BERTweet's limit of 128."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Embed the Tokenized Samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw1_new_kernel",
   "language": "python",
   "name": "cw1_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
