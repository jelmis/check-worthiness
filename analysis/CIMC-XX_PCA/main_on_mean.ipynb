{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PCA\n",
    "In this notebook, we conduct a PCA on our mean features. We want to check if the explained variance of the principal components fit our hypothesis that the image embeddings do not add any significant information.\n",
    "\n",
    "We will conduct a PCA for\n",
    "- the training split\n",
    "- all splits combined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports and Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.173018717Z",
     "start_time": "2023-06-12T08:49:01.551853233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# AUTORELOAD\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# GENERAL IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TASK-SPECIFIC IMPORTS\n",
    "from src import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CONSTANTS\n",
    "users = [\"patriziopalmisano\", \"onurdenizguler\", \"jockl\"]\n",
    "TRAIN = \"train\"\n",
    "DEV = \"dev\"\n",
    "TEST = \"test\"\n",
    "\n",
    "####################### SELECT ###########################\n",
    "user = users[2] # SELECT USER\n",
    "version = \"v2\" # SELECT DATASET VERSION\n",
    "dataset_version = version\n",
    "##########################################################\n",
    "\n",
    "if user in users[:2]:\n",
    "    data_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/data/CT23_1A_checkworthy_multimodal_english_{version}\"\n",
    "    cw_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/\"\n",
    "\n",
    "else:\n",
    "    data_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_{dataset_version}\"\n",
    "    cw_dir = \"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive\"\n",
    "\n",
    "features_dir = f\"{data_dir}/features\"\n",
    "labels_dir = f\"{data_dir}/labels\"\n",
    "models_dir = f\"{cw_dir}/models/vanillann\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Train Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Load Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first load all the features and compare their shapes and contents with the original embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train txt embeddings: (2356, 768)\n",
      "Train img embeddings: (2356, 768)\n",
      "Train mean features: (2356, 768)\n"
     ]
    }
   ],
   "source": [
    "train_txt_emb, train_img_emb = utils.get_embeddings_from_pickle_file(f\"{data_dir}/embeddings_{TRAIN}_{dataset_version}.pickle\")\n",
    "train_mean_features = np.load(f\"{features_dir}/mean/mean_{TRAIN}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "print(f\"Train txt embeddings: {train_txt_emb.shape}\")\n",
    "print(f\"Train img embeddings: {train_img_emb.shape}\")\n",
    "print(f\"Train mean features: {train_mean_features.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.178152380Z",
     "start_time": "2023-06-12T08:49:01.593152299Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spot check:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train txt embd excerpt: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Train img embd excerpt: [-0.24858032  0.6837659   0.81424457  0.59864545  0.49090493]\n",
      "Train features excerpt: [ 0.03200077  0.77299803  0.30925897  0.5077729  -0.16982862]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train txt embd excerpt: {train_txt_emb[0][:5]}\")\n",
    "print(f\"Train img embd excerpt: {train_img_emb[0][:5]}\")\n",
    "print(f\"Train features excerpt: {train_mean_features[0][:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.182698176Z",
     "start_time": "2023-06-12T08:49:01.614368006Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Normalize the Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform a PCA, we first need to normalize the feature values. The normalized features should have a mean of 0, and standard deviation of 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train normalized mean features: (2356, 768)\n",
      "Mean: -1.0541285726251015e-11\n",
      "Standard Deviation: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "train_normalized_mean_features = StandardScaler().fit_transform(train_mean_features)\n",
    "print(f\"Train normalized mean features: {train_normalized_mean_features.shape}\")\n",
    "print(f\"Mean: {np.mean(train_normalized_mean_features)}\")\n",
    "print(f\"Standard Deviation: {np.std(train_normalized_mean_features)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.211119741Z",
     "start_time": "2023-06-12T08:49:01.639412832Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean and standard deviation have the desired values, the features are now normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 PCA and Explained Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have normalized feature values, we can compute all principal components.\n",
    "\n",
    "IMPORTANT NOTE: There is no direct \"mapping\" between the n-th PC and the n-th feature dimension. The PCs are strictly ordered according to their explained variance values - by definition, the first PC explains the highest amount of variance, while this of course does not have to be the case for the first feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2         3         4         5         6    \\\n2351 -5.618376  11.033623   6.053107 -0.163067  0.666468 -1.900198  3.436073   \n2352 -4.682307 -12.436971  11.926312 -2.077713 -0.727863 -2.583416 -4.341856   \n2353 -6.491424   5.327140  -1.383168  0.875179 -2.787508  0.841726  0.396089   \n2354 -4.693285 -10.822100  12.317378  1.285923 -9.692060 -0.570250 -1.636979   \n2355 -3.795097 -10.701345   7.032280  0.255650 -2.192058 -2.775885  0.135702   \n\n           7         8         9    ...       758       759       760  \\\n2351 -3.184094  1.393384 -0.178092  ...  0.009635 -0.015563  0.002220   \n2352 -4.070567 -1.320721 -0.866155  ...  0.006398  0.001851 -0.008635   \n2353  1.701471  2.941340 -2.521940  ...  0.001634  0.008995 -0.014275   \n2354  3.693459  4.632005 -3.306872  ...  0.011457  0.007084 -0.002748   \n2355 -0.844129  1.474199  0.036058  ... -0.006232 -0.015062 -0.014292   \n\n           761       762       763       764       765       766       767  \n2351 -0.006839  0.015748  0.008607 -0.003422 -0.009546  0.008130  0.001067  \n2352 -0.011726  0.014429  0.008191 -0.002672  0.007399 -0.002513 -0.001518  \n2353 -0.000268  0.022956 -0.009280 -0.005194  0.008960  0.011898 -0.000954  \n2354  0.012913  0.007749 -0.023125  0.005615 -0.001070 -0.006236  0.000914  \n2355 -0.004327 -0.001183 -0.024918  0.018230  0.001745 -0.010218 -0.000668  \n\n[5 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2351</th>\n      <td>-5.618376</td>\n      <td>11.033623</td>\n      <td>6.053107</td>\n      <td>-0.163067</td>\n      <td>0.666468</td>\n      <td>-1.900198</td>\n      <td>3.436073</td>\n      <td>-3.184094</td>\n      <td>1.393384</td>\n      <td>-0.178092</td>\n      <td>...</td>\n      <td>0.009635</td>\n      <td>-0.015563</td>\n      <td>0.002220</td>\n      <td>-0.006839</td>\n      <td>0.015748</td>\n      <td>0.008607</td>\n      <td>-0.003422</td>\n      <td>-0.009546</td>\n      <td>0.008130</td>\n      <td>0.001067</td>\n    </tr>\n    <tr>\n      <th>2352</th>\n      <td>-4.682307</td>\n      <td>-12.436971</td>\n      <td>11.926312</td>\n      <td>-2.077713</td>\n      <td>-0.727863</td>\n      <td>-2.583416</td>\n      <td>-4.341856</td>\n      <td>-4.070567</td>\n      <td>-1.320721</td>\n      <td>-0.866155</td>\n      <td>...</td>\n      <td>0.006398</td>\n      <td>0.001851</td>\n      <td>-0.008635</td>\n      <td>-0.011726</td>\n      <td>0.014429</td>\n      <td>0.008191</td>\n      <td>-0.002672</td>\n      <td>0.007399</td>\n      <td>-0.002513</td>\n      <td>-0.001518</td>\n    </tr>\n    <tr>\n      <th>2353</th>\n      <td>-6.491424</td>\n      <td>5.327140</td>\n      <td>-1.383168</td>\n      <td>0.875179</td>\n      <td>-2.787508</td>\n      <td>0.841726</td>\n      <td>0.396089</td>\n      <td>1.701471</td>\n      <td>2.941340</td>\n      <td>-2.521940</td>\n      <td>...</td>\n      <td>0.001634</td>\n      <td>0.008995</td>\n      <td>-0.014275</td>\n      <td>-0.000268</td>\n      <td>0.022956</td>\n      <td>-0.009280</td>\n      <td>-0.005194</td>\n      <td>0.008960</td>\n      <td>0.011898</td>\n      <td>-0.000954</td>\n    </tr>\n    <tr>\n      <th>2354</th>\n      <td>-4.693285</td>\n      <td>-10.822100</td>\n      <td>12.317378</td>\n      <td>1.285923</td>\n      <td>-9.692060</td>\n      <td>-0.570250</td>\n      <td>-1.636979</td>\n      <td>3.693459</td>\n      <td>4.632005</td>\n      <td>-3.306872</td>\n      <td>...</td>\n      <td>0.011457</td>\n      <td>0.007084</td>\n      <td>-0.002748</td>\n      <td>0.012913</td>\n      <td>0.007749</td>\n      <td>-0.023125</td>\n      <td>0.005615</td>\n      <td>-0.001070</td>\n      <td>-0.006236</td>\n      <td>0.000914</td>\n    </tr>\n    <tr>\n      <th>2355</th>\n      <td>-3.795097</td>\n      <td>-10.701345</td>\n      <td>7.032280</td>\n      <td>0.255650</td>\n      <td>-2.192058</td>\n      <td>-2.775885</td>\n      <td>0.135702</td>\n      <td>-0.844129</td>\n      <td>1.474199</td>\n      <td>0.036058</td>\n      <td>...</td>\n      <td>-0.006232</td>\n      <td>-0.015062</td>\n      <td>-0.014292</td>\n      <td>-0.004327</td>\n      <td>-0.001183</td>\n      <td>-0.024918</td>\n      <td>0.018230</td>\n      <td>0.001745</td>\n      <td>-0.010218</td>\n      <td>-0.000668</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pca = PCA()\n",
    "train_principal_components = train_pca.fit_transform(train_normalized_mean_features)\n",
    "train_principal_components_df = pd.DataFrame(train_principal_components)\n",
    "train_principal_components_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.467224261Z",
     "start_time": "2023-06-12T08:49:01.698793856Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity Check: Does the explained variance array have the right shape, do the values add up to 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance array shape: (768,)\n",
      "Sum of all explained variance values: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "train_explained_variance = train_pca.explained_variance_ratio_\n",
    "sum_of_train_explained_variance_values = np.sum(train_explained_variance)\n",
    "print(f\"Explained variance array shape: {train_explained_variance.shape}\")\n",
    "print(f\"Sum of all explained variance values: {sum_of_train_explained_variance_values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.467785425Z",
     "start_time": "2023-06-12T08:49:02.428642608Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have the explained variance for all the 768 principal components. Let’s now sum over the first and last 384 principal components:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the first 768 PCs within train split: 0.913522481918335\n",
      "Explained variance of the last 768 PCs within train split: 0.0864773839712143\n"
     ]
    }
   ],
   "source": [
    "train_expl_var_of_first_half_of_pcs = np.sum(train_explained_variance[:384])\n",
    "train_expl_var_of_second_half_of_pcs = np.sum(train_explained_variance[-384:])\n",
    "print(f\"Explained variance of the first 768 PCs within train split: {train_expl_var_of_first_half_of_pcs}\")\n",
    "print(f\"Explained variance of the last 768 PCs within train split: {train_expl_var_of_second_half_of_pcs}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.478993008Z",
     "start_time": "2023-06-12T08:49:02.445270931Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. All Splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Load Features\n",
    "Let's first load all the features and compare their shapes and contents with the original embeddings. We want to make sure that the first 768 feature dimensions indeed belong to the text embeddings and the last 768 to the image embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All mean features: (3175, 768)\n"
     ]
    }
   ],
   "source": [
    "dev_mean_features = np.load(f\"{features_dir}/mean/mean_{DEV}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "test_mean_features = np.load(f\"{features_dir}/mean/mean_{TEST}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "all_mean_features = np.concatenate((train_mean_features, dev_mean_features, test_mean_features))\n",
    "print(f\"All mean features: {all_mean_features.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.559607830Z",
     "start_time": "2023-06-12T08:49:02.479280154Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spot check:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test txt embd excerpt: [ 0.6375199  -0.53175956  0.20533158 -0.59946156  0.6934856 ]\n",
      "Test img embd excerpt: [ 0.4965897  -0.18978584  0.3129852  -1.5566363  -0.03816223]\n",
      "Test features excerpt: [ 0.5670548 -0.3607727  0.2591584 -1.078049   0.3276617]\n"
     ]
    }
   ],
   "source": [
    "# Load test embeddings\n",
    "test_txt_emb, test_img_emb = utils.get_embeddings_from_pickle_file(f\"{data_dir}/embeddings_{TEST}_{dataset_version}.pickle\")\n",
    "\n",
    "# Spot check\n",
    "print(f\"Test txt embd excerpt: {test_txt_emb[-1][:5]}\")\n",
    "print(f\"Test img embd excerpt: {test_img_emb[-1][:5]}\")\n",
    "print(f\"Test features excerpt: {all_mean_features[-1][:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.581123477Z",
     "start_time": "2023-06-12T08:49:02.496324638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Normalize the Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform a PCA, we first need to normalize the feature values. The normalized features should have a mean of 0, and standard deviation of 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev normalized mean features: (3175, 768)\n",
      "Mean: -1.8147346125818586e-10\n",
      "Standard Deviation: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "all_normalized_mean_features = StandardScaler().fit_transform(all_mean_features)\n",
    "print(f\"Dev normalized mean features: {all_normalized_mean_features.shape}\")\n",
    "print(f\"Mean: {np.mean(all_normalized_mean_features)}\")\n",
    "print(f\"Standard Deviation: {np.std(all_normalized_mean_features)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:02.760631260Z",
     "start_time": "2023-06-12T08:49:02.523611019Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean and standard deviation have the desired values, the features are now normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 PCA and Explained Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have normalized feature values, we can compute all principal components.\n",
    "\n",
    "IMPORTANT NOTE: There is no direct \"mapping\" between the n-th PC and the n-th feature dimension. The PCs are strictly ordered according to their explained variance values - by definition, the first PC explains the highest amount of variance, while this of course does not have to be the case for the first feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "           0          1          2         3         4         5         6    \\\n3170  0.238008  -4.663329   2.159748  3.730483  4.448696 -1.656778 -0.004506   \n3171 -3.178096 -12.321038  11.737628  3.931248 -1.461085 -4.500793 -4.868284   \n3172 -2.032893  -2.270451  -3.973131 -0.067350 -2.996502  1.322557  2.729575   \n3173 -3.744528  -4.235510  -7.302965  0.158635 -2.333645  2.705621 -0.197544   \n3174 -3.745104  -2.916368  -4.660560  1.345548  0.948931  0.764762 -2.684519   \n\n           7         8         9    ...       758       759       760  \\\n3170 -0.165600 -1.452704 -1.266812  ... -0.008645  0.022458  0.010699   \n3171  2.704535  0.289328 -0.964185  ...  0.007725  0.015567 -0.000813   \n3172 -0.248537  0.513258 -2.773205  ...  0.003104 -0.004439 -0.006665   \n3173 -1.512575 -0.685376 -1.014182  ...  0.014284 -0.000860  0.018936   \n3174  2.324849  1.664622 -1.585021  ... -0.002761 -0.004715 -0.001393   \n\n           761       762       763       764       765       766       767  \n3170  0.003758  0.007940 -0.010041  0.012951  0.000659 -0.018835 -0.001551  \n3171  0.011691  0.011216 -0.008845 -0.008537  0.002350 -0.007344 -0.000480  \n3172 -0.004283 -0.003620 -0.012886  0.000682 -0.008256 -0.006176  0.000802  \n3173 -0.028383 -0.018947 -0.018295 -0.008542 -0.002742  0.020855 -0.000874  \n3174 -0.003119 -0.001860  0.006264  0.011540  0.004378 -0.001430  0.003327  \n\n[5 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3170</th>\n      <td>0.238008</td>\n      <td>-4.663329</td>\n      <td>2.159748</td>\n      <td>3.730483</td>\n      <td>4.448696</td>\n      <td>-1.656778</td>\n      <td>-0.004506</td>\n      <td>-0.165600</td>\n      <td>-1.452704</td>\n      <td>-1.266812</td>\n      <td>...</td>\n      <td>-0.008645</td>\n      <td>0.022458</td>\n      <td>0.010699</td>\n      <td>0.003758</td>\n      <td>0.007940</td>\n      <td>-0.010041</td>\n      <td>0.012951</td>\n      <td>0.000659</td>\n      <td>-0.018835</td>\n      <td>-0.001551</td>\n    </tr>\n    <tr>\n      <th>3171</th>\n      <td>-3.178096</td>\n      <td>-12.321038</td>\n      <td>11.737628</td>\n      <td>3.931248</td>\n      <td>-1.461085</td>\n      <td>-4.500793</td>\n      <td>-4.868284</td>\n      <td>2.704535</td>\n      <td>0.289328</td>\n      <td>-0.964185</td>\n      <td>...</td>\n      <td>0.007725</td>\n      <td>0.015567</td>\n      <td>-0.000813</td>\n      <td>0.011691</td>\n      <td>0.011216</td>\n      <td>-0.008845</td>\n      <td>-0.008537</td>\n      <td>0.002350</td>\n      <td>-0.007344</td>\n      <td>-0.000480</td>\n    </tr>\n    <tr>\n      <th>3172</th>\n      <td>-2.032893</td>\n      <td>-2.270451</td>\n      <td>-3.973131</td>\n      <td>-0.067350</td>\n      <td>-2.996502</td>\n      <td>1.322557</td>\n      <td>2.729575</td>\n      <td>-0.248537</td>\n      <td>0.513258</td>\n      <td>-2.773205</td>\n      <td>...</td>\n      <td>0.003104</td>\n      <td>-0.004439</td>\n      <td>-0.006665</td>\n      <td>-0.004283</td>\n      <td>-0.003620</td>\n      <td>-0.012886</td>\n      <td>0.000682</td>\n      <td>-0.008256</td>\n      <td>-0.006176</td>\n      <td>0.000802</td>\n    </tr>\n    <tr>\n      <th>3173</th>\n      <td>-3.744528</td>\n      <td>-4.235510</td>\n      <td>-7.302965</td>\n      <td>0.158635</td>\n      <td>-2.333645</td>\n      <td>2.705621</td>\n      <td>-0.197544</td>\n      <td>-1.512575</td>\n      <td>-0.685376</td>\n      <td>-1.014182</td>\n      <td>...</td>\n      <td>0.014284</td>\n      <td>-0.000860</td>\n      <td>0.018936</td>\n      <td>-0.028383</td>\n      <td>-0.018947</td>\n      <td>-0.018295</td>\n      <td>-0.008542</td>\n      <td>-0.002742</td>\n      <td>0.020855</td>\n      <td>-0.000874</td>\n    </tr>\n    <tr>\n      <th>3174</th>\n      <td>-3.745104</td>\n      <td>-2.916368</td>\n      <td>-4.660560</td>\n      <td>1.345548</td>\n      <td>0.948931</td>\n      <td>0.764762</td>\n      <td>-2.684519</td>\n      <td>2.324849</td>\n      <td>1.664622</td>\n      <td>-1.585021</td>\n      <td>...</td>\n      <td>-0.002761</td>\n      <td>-0.004715</td>\n      <td>-0.001393</td>\n      <td>-0.003119</td>\n      <td>-0.001860</td>\n      <td>0.006264</td>\n      <td>0.011540</td>\n      <td>0.004378</td>\n      <td>-0.001430</td>\n      <td>0.003327</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pca = PCA()\n",
    "all_principal_components = all_pca.fit_transform(all_normalized_mean_features)\n",
    "all_principal_components_df = pd.DataFrame(all_principal_components)\n",
    "all_principal_components_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:03.706125174Z",
     "start_time": "2023-06-12T08:49:02.601995315Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity Check: Has the explained variance array the right shape, do the values add up to 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance array shape: (768,)\n",
      "Sum of all explained variance values: 1.0\n"
     ]
    }
   ],
   "source": [
    "all_explained_variance = all_pca.explained_variance_ratio_\n",
    "all_sum_of_explained_variance_values = np.sum(all_explained_variance)\n",
    "print(f\"Explained variance array shape: {all_explained_variance.shape}\")\n",
    "print(f\"Sum of all explained variance values: {all_sum_of_explained_variance_values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:03.722679024Z",
     "start_time": "2023-06-12T08:49:03.706482877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have the explained variance for all the 768 principal components. Let’s now sum over the first and last 384 principal components:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the first 768 PCs within all splits: 0.9037585258483887\n",
      "Explained variance of the last 768 PCs within all splits: 0.0962415337562561\n"
     ]
    }
   ],
   "source": [
    "expl_var_of_first_half_of_pcs = np.sum(all_explained_variance[:384])\n",
    "expl_var_of_second_half_of_pcs = np.sum(all_explained_variance[-384:])\n",
    "print(f\"Explained variance of the first 768 PCs within all splits: {expl_var_of_first_half_of_pcs}\")\n",
    "print(f\"Explained variance of the last 768 PCs within all splits: {expl_var_of_second_half_of_pcs}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:03.809619437Z",
     "start_time": "2023-06-12T08:49:03.722111844Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Summary of Results and Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results for train split and all data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the first 384 PCs within train split: 0.913522481918335\n",
      "Explained variance of the second 384 PCs within train split: 0.0864773839712143\n",
      "\n",
      "Explained variance of the first 384 PCs within all splits: 0.9037585258483887\n",
      "Explained variance of the second 384 PCs within all splits: 0.0962415337562561\n"
     ]
    }
   ],
   "source": [
    "print(f\"Explained variance of the first 384 PCs within train split: {train_expl_var_of_first_half_of_pcs}\")\n",
    "print(f\"Explained variance of the second 384 PCs within train split: {train_expl_var_of_second_half_of_pcs}\\n\")\n",
    "\n",
    "print(f\"Explained variance of the first 384 PCs within all splits: {expl_var_of_first_half_of_pcs}\")\n",
    "print(f\"Explained variance of the second 384 PCs within all splits: {expl_var_of_second_half_of_pcs}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:49:03.822628744Z",
     "start_time": "2023-06-12T08:49:03.752174001Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The first half of PCs capture around 90 % of the features' variance.\n",
    "- In comparison, for the concat features, the first half of PCs capture around 95 % of the feature's variance.\n",
    "- This matches our previous findings: The text features are way more important than the image features."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cw1_kernel",
   "language": "python",
   "display_name": "cw1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
