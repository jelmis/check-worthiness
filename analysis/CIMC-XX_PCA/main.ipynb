{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PCA\n",
    "In this notebook, we conduct a PCA on our concat features. We want to check if the explained variance of the principal components fit our hypothesis that the image embeddings do not add any significant information.\n",
    "\n",
    "We will conduct a PCA for\n",
    "- the training split\n",
    "- all splits combined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports and Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:15.691810276Z",
     "start_time": "2023-06-02T09:54:14.483365003Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUTORELOAD\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# GENERAL IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TASK-SPECIFIC IMPORTS\n",
    "from src import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CONSTANTS\n",
    "users = [\"patriziopalmisano\", \"onurdenizguler\", \"jockl\"]\n",
    "TRAIN = \"train\"\n",
    "DEV = \"dev\"\n",
    "TEST = \"test\"\n",
    "\n",
    "####################### SELECT ###########################\n",
    "user = users[2] # SELECT USER\n",
    "version = \"v2\" # SELECT DATASET VERSION\n",
    "dataset_version = version\n",
    "##########################################################\n",
    "\n",
    "if user in users[:2]:\n",
    "    data_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/data/CT23_1A_checkworthy_multimodal_english_{version}\"\n",
    "    cw_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/\"\n",
    "\n",
    "else:\n",
    "    data_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_{dataset_version}\"\n",
    "    cw_dir = \"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive\"\n",
    "\n",
    "features_dir = f\"{data_dir}/features\"\n",
    "labels_dir = f\"{data_dir}/labels\"\n",
    "models_dir = f\"{cw_dir}/models/vanillann\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Train Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Load Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first load all the features and compare their shapes and contents with the original embeddings. We want to make sure that the first 768 feature dimensions indeed belong to the text embeddings and the last 768 to the image embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train txt embeddings: (2356, 768)\n",
      "Train img embeddings: (2356, 768)\n",
      "Train concat features: (2356, 1536)\n"
     ]
    }
   ],
   "source": [
    "train_txt_emb, train_img_emb = utils.get_embeddings_from_pickle_file(f\"{data_dir}/embeddings_{TRAIN}_{dataset_version}.pickle\")\n",
    "train_concat_features = np.load(f\"{features_dir}/concat/concat_{TRAIN}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "print(f\"Train txt embeddings: {train_txt_emb.shape}\")\n",
    "print(f\"Train img embeddings: {train_img_emb.shape}\")\n",
    "print(f\"Train concat features: {train_concat_features.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:22.027609739Z",
     "start_time": "2023-06-02T09:54:21.901789892Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spot check if the first 768 feature dimensions indeed belong to the text embeddings, the latter 768 to the image embeddings:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train txt embd excerpt: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Train features excerpt: [ 0.31258187  0.8622302  -0.19572662  0.41690043 -0.8305622 ]\n",
      "Train img embd excerpt: [ 0.20370999  0.39563796 -0.41939157 -0.35091972  0.02099419]\n",
      "Train features excerpt: [ 0.20370999  0.39563796 -0.41939157 -0.35091972  0.02099419]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train txt embd excerpt: {train_txt_emb[0][:5]}\")\n",
    "print(f\"Train features excerpt: {train_concat_features[0][:5]}\")\n",
    "print(f\"Train img embd excerpt: {train_img_emb[0][-5:]}\")\n",
    "print(f\"Train features excerpt: {train_concat_features[0][-5:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:25.223702673Z",
     "start_time": "2023-06-02T09:54:25.186539826Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Normalize the Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform a PCA, we first need to normalize the feature values. The normalized features should have a mean of 0, and standard deviation of 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train normalized concat features: (2356, 1536)\n",
      "Mean: -3.0358901503824143e-10\n",
      "Standard Deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "train_normalized_concat_features = StandardScaler().fit_transform(train_concat_features)\n",
    "print(f\"Train normalized concat features: {train_normalized_concat_features.shape}\")\n",
    "print(f\"Mean: {np.mean(train_normalized_concat_features)}\")\n",
    "print(f\"Standard Deviation: {np.std(train_normalized_concat_features)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:26.712780780Z",
     "start_time": "2023-06-02T09:54:26.501579185Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean and standard deviation have the desired values, the features are now normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 PCA and Explained Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have normalized feature values, we can compute all principal components.\n",
    "\n",
    "IMPORTANT NOTE: There is no direct \"mapping\" between the n-th PC and the n-th feature dimension. The PCs are strictly ordered according to their explained variance values - by definition, the first PC explains the highest amount of variance, while this of course does not have to be the case for the first feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "          0          1          2         3          4          5     \\\n2351 -6.303586 -10.861704   8.986289  0.001850   1.282223   4.173169   \n2352 -5.564433  15.178924  15.095158 -1.164485   1.757364   7.351459   \n2353 -8.414557  -7.164299  -3.135854  1.749057   2.090608   0.284795   \n2354 -6.685371  13.612117   5.393781 -2.223035  14.665219  10.999657   \n2355 -4.898365  12.677189   4.458490 -3.420100   4.512700   5.863922   \n\n          6         7         8         9     ...      1526      1527  \\\n2351  1.159974 -8.352036  2.054846 -0.449837  ... -0.000048  0.000346   \n2352  4.915713  2.866526 -2.022675 -0.501234  ... -0.000185 -0.000296   \n2353 -1.228448  0.205026 -0.805630 -0.400631  ... -0.000015 -0.001139   \n2354 -3.644086  6.255527 -3.607357 -2.052888  ... -0.000020  0.001407   \n2355 -2.172963  4.598461 -3.228651  5.265623  ... -0.000161  0.000900   \n\n          1528      1529      1530      1531      1532      1533      1534  \\\n2351  0.000732  0.000309 -0.000086  0.000094  0.000246  0.000036 -0.000005   \n2352 -0.000203  0.000185 -0.000216  0.000020  0.000196  0.000328 -0.000095   \n2353  0.000267  0.000211  0.000527  0.000028 -0.000058 -0.000009  0.000033   \n2354 -0.000715 -0.000512 -0.000194  0.001287  0.000310  0.000221  0.000010   \n2355  0.000517 -0.000776 -0.000191  0.000158  0.000248 -0.000141 -0.000029   \n\n              1535  \n2351  1.306930e-06  \n2352 -5.680644e-07  \n2353  4.301229e-07  \n2354  2.617451e-06  \n2355  3.767290e-06  \n\n[5 rows x 1536 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1526</th>\n      <th>1527</th>\n      <th>1528</th>\n      <th>1529</th>\n      <th>1530</th>\n      <th>1531</th>\n      <th>1532</th>\n      <th>1533</th>\n      <th>1534</th>\n      <th>1535</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2351</th>\n      <td>-6.303586</td>\n      <td>-10.861704</td>\n      <td>8.986289</td>\n      <td>0.001850</td>\n      <td>1.282223</td>\n      <td>4.173169</td>\n      <td>1.159974</td>\n      <td>-8.352036</td>\n      <td>2.054846</td>\n      <td>-0.449837</td>\n      <td>...</td>\n      <td>-0.000048</td>\n      <td>0.000346</td>\n      <td>0.000732</td>\n      <td>0.000309</td>\n      <td>-0.000086</td>\n      <td>0.000094</td>\n      <td>0.000246</td>\n      <td>0.000036</td>\n      <td>-0.000005</td>\n      <td>1.306930e-06</td>\n    </tr>\n    <tr>\n      <th>2352</th>\n      <td>-5.564433</td>\n      <td>15.178924</td>\n      <td>15.095158</td>\n      <td>-1.164485</td>\n      <td>1.757364</td>\n      <td>7.351459</td>\n      <td>4.915713</td>\n      <td>2.866526</td>\n      <td>-2.022675</td>\n      <td>-0.501234</td>\n      <td>...</td>\n      <td>-0.000185</td>\n      <td>-0.000296</td>\n      <td>-0.000203</td>\n      <td>0.000185</td>\n      <td>-0.000216</td>\n      <td>0.000020</td>\n      <td>0.000196</td>\n      <td>0.000328</td>\n      <td>-0.000095</td>\n      <td>-5.680644e-07</td>\n    </tr>\n    <tr>\n      <th>2353</th>\n      <td>-8.414557</td>\n      <td>-7.164299</td>\n      <td>-3.135854</td>\n      <td>1.749057</td>\n      <td>2.090608</td>\n      <td>0.284795</td>\n      <td>-1.228448</td>\n      <td>0.205026</td>\n      <td>-0.805630</td>\n      <td>-0.400631</td>\n      <td>...</td>\n      <td>-0.000015</td>\n      <td>-0.001139</td>\n      <td>0.000267</td>\n      <td>0.000211</td>\n      <td>0.000527</td>\n      <td>0.000028</td>\n      <td>-0.000058</td>\n      <td>-0.000009</td>\n      <td>0.000033</td>\n      <td>4.301229e-07</td>\n    </tr>\n    <tr>\n      <th>2354</th>\n      <td>-6.685371</td>\n      <td>13.612117</td>\n      <td>5.393781</td>\n      <td>-2.223035</td>\n      <td>14.665219</td>\n      <td>10.999657</td>\n      <td>-3.644086</td>\n      <td>6.255527</td>\n      <td>-3.607357</td>\n      <td>-2.052888</td>\n      <td>...</td>\n      <td>-0.000020</td>\n      <td>0.001407</td>\n      <td>-0.000715</td>\n      <td>-0.000512</td>\n      <td>-0.000194</td>\n      <td>0.001287</td>\n      <td>0.000310</td>\n      <td>0.000221</td>\n      <td>0.000010</td>\n      <td>2.617451e-06</td>\n    </tr>\n    <tr>\n      <th>2355</th>\n      <td>-4.898365</td>\n      <td>12.677189</td>\n      <td>4.458490</td>\n      <td>-3.420100</td>\n      <td>4.512700</td>\n      <td>5.863922</td>\n      <td>-2.172963</td>\n      <td>4.598461</td>\n      <td>-3.228651</td>\n      <td>5.265623</td>\n      <td>...</td>\n      <td>-0.000161</td>\n      <td>0.000900</td>\n      <td>0.000517</td>\n      <td>-0.000776</td>\n      <td>-0.000191</td>\n      <td>0.000158</td>\n      <td>0.000248</td>\n      <td>-0.000141</td>\n      <td>-0.000029</td>\n      <td>3.767290e-06</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1536 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pca = PCA()\n",
    "train_principal_components = train_pca.fit_transform(train_normalized_concat_features)\n",
    "train_principal_components_df = pd.DataFrame(train_principal_components)\n",
    "train_principal_components_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:34.761585632Z",
     "start_time": "2023-06-02T09:54:28.064582518Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity Check: Does the explained variance array have the right shape, do the values add up to 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance array shape: (1536,)\n",
      "Sum of all explained variance values: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "train_explained_variance = train_pca.explained_variance_ratio_\n",
    "sum_of_train_explained_variance_values = np.sum(train_explained_variance)\n",
    "print(f\"Explained variance array shape: {train_explained_variance.shape}\")\n",
    "print(f\"Sum of all explained variance values: {sum_of_train_explained_variance_values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T10:06:08.717844675Z",
     "start_time": "2023-06-02T10:06:08.654919615Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have the explained variance for all the 1536 principal components. Let's now sum over the first and last 768 values:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the first 768 PCs within train split: 0.9474944472312927\n",
      "Explained variance of the last 768 PCs within train split: 0.052505627274513245\n"
     ]
    }
   ],
   "source": [
    "train_txt_features_explained_variance = np.sum(train_explained_variance[:train_txt_emb.shape[1]])\n",
    "train_img_features_explained_variance = np.sum(train_explained_variance[-train_img_emb.shape[1]:])\n",
    "print(f\"Explained variance of the first 768 PCs within train split: {train_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of the last 768 PCs within train split: {train_img_features_explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:34.859897588Z",
     "start_time": "2023-06-02T09:54:34.745189192Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. All Splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Load Features\n",
    "Let's first load all the features and compare their shapes and contents with the original embeddings. We want to make sure that the first 768 feature dimensions indeed belong to the text embeddings and the last 768 to the image embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All concat features: (3175, 1536)\n"
     ]
    }
   ],
   "source": [
    "dev_concat_features = np.load(f\"{features_dir}/concat/concat_{DEV}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "test_concat_features = np.load(f\"{features_dir}/concat/concat_{TEST}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "all_concat_features = np.concatenate((train_concat_features, dev_concat_features, test_concat_features))\n",
    "print(f\"All concat features: {all_concat_features.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:34.861995791Z",
     "start_time": "2023-06-02T09:54:34.785727545Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spot check if the first 768 feature dimensions indeed belong to the text embeddings, the latter 768 to the image embeddings:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test txt embd excerpt: [ 0.6375199  -0.53175956  0.20533158 -0.59946156  0.6934856 ]\n",
      "Test features excerpt: [ 0.6375199  -0.53175956  0.20533158 -0.59946156  0.6934856 ]\n",
      "Test img embd excerpt: [-0.6128674   0.13787843 -0.59581023  0.30937177  0.3236546 ]\n",
      "Test features excerpt: [-0.6128674   0.13787843 -0.59581023  0.30937177  0.3236546 ]\n"
     ]
    }
   ],
   "source": [
    "# Load test embeddings\n",
    "test_txt_emb, test_img_emb = utils.get_embeddings_from_pickle_file(f\"{data_dir}/embeddings_{TEST}_{dataset_version}.pickle\")\n",
    "\n",
    "# Spot check\n",
    "print(f\"Test txt embd excerpt: {test_txt_emb[-1][:5]}\")\n",
    "print(f\"Test features excerpt: {all_concat_features[-1][:5]}\")\n",
    "print(f\"Test img embd excerpt: {test_img_emb[-1][-5:]}\")\n",
    "print(f\"Test features excerpt: {all_concat_features[-1][-5:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:35.089140725Z",
     "start_time": "2023-06-02T09:54:34.829192261Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Normalize the Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform a PCA, we first need to normalize the feature values. The normalized features should have a mean of 0, and standard deviation of 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev normalized concat features: (3175, 1536)\n",
      "Mean: 1.6739362673767744e-10\n",
      "Standard Deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "all_normalized_concat_features = StandardScaler().fit_transform(all_concat_features)\n",
    "print(f\"Dev normalized concat features: {all_normalized_concat_features.shape}\")\n",
    "print(f\"Mean: {np.mean(all_normalized_concat_features)}\")\n",
    "print(f\"Standard Deviation: {np.std(all_normalized_concat_features)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:36.203746800Z",
     "start_time": "2023-06-02T09:54:34.849005935Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean and standard deviation have the desired values, the features are now normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 PCA and Explained Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have normalized feature values, we can compute all principal components.\n",
    "\n",
    "IMPORTANT NOTE: There is no direct \"mapping\" between the n-th PC and the n-th feature dimension. The PCs are strictly ordered according to their explained variance values - by definition, the first PC explains the highest amount of variance, while this of course does not have to be the case for the first feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "          0          1          2          3         4         5         6     \\\n3170 -0.509420  -6.539375   3.930840  -6.451927 -5.348428 -2.384798 -0.779864   \n3171 -4.458532 -14.868099  10.573829 -10.466908  0.568076  6.589172  2.355393   \n3172 -4.387179  -4.060971  -7.321678   2.225588  2.264590  1.069938  0.334720   \n3173 -4.627469  -6.619272  -8.359013   4.613657  0.688303 -0.457603 -1.968335   \n3174 -5.531907  -3.422538  -4.801727  -0.755215 -3.030000 -4.084527  0.745827   \n\n          7         8         9     ...      1526      1527      1528  \\\n3170 -5.852808  2.231250 -3.941541  ...  0.001402 -0.000920 -0.000160   \n3171  7.669797 -1.359096 -4.657529  ...  0.000227 -0.000316  0.000413   \n3172 -3.195267 -2.207384 -5.335334  ...  0.000872 -0.000283 -0.000864   \n3173 -3.471986  0.361107 -5.644895  ... -0.000321 -0.000440  0.001092   \n3174  2.085324 -1.728323 -0.899399  ...  0.000189 -0.000214 -0.000448   \n\n          1529      1530      1531      1532      1533      1534          1535  \n3170  0.000540 -0.000198  0.000056  0.000084  0.000050 -0.000023 -1.196255e-06  \n3171  0.000144  0.000721 -0.000532  0.000470 -0.000016 -0.000009 -3.934614e-06  \n3172 -0.000395 -0.000498 -0.000170  0.000033 -0.000263 -0.000018 -3.317778e-06  \n3173 -0.000409 -0.000395 -0.000395  0.000123  0.000199  0.000023 -1.768249e-06  \n3174  0.000365  0.000415  0.000190 -0.000121 -0.000129 -0.000018 -5.678798e-07  \n\n[5 rows x 1536 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1526</th>\n      <th>1527</th>\n      <th>1528</th>\n      <th>1529</th>\n      <th>1530</th>\n      <th>1531</th>\n      <th>1532</th>\n      <th>1533</th>\n      <th>1534</th>\n      <th>1535</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3170</th>\n      <td>-0.509420</td>\n      <td>-6.539375</td>\n      <td>3.930840</td>\n      <td>-6.451927</td>\n      <td>-5.348428</td>\n      <td>-2.384798</td>\n      <td>-0.779864</td>\n      <td>-5.852808</td>\n      <td>2.231250</td>\n      <td>-3.941541</td>\n      <td>...</td>\n      <td>0.001402</td>\n      <td>-0.000920</td>\n      <td>-0.000160</td>\n      <td>0.000540</td>\n      <td>-0.000198</td>\n      <td>0.000056</td>\n      <td>0.000084</td>\n      <td>0.000050</td>\n      <td>-0.000023</td>\n      <td>-1.196255e-06</td>\n    </tr>\n    <tr>\n      <th>3171</th>\n      <td>-4.458532</td>\n      <td>-14.868099</td>\n      <td>10.573829</td>\n      <td>-10.466908</td>\n      <td>0.568076</td>\n      <td>6.589172</td>\n      <td>2.355393</td>\n      <td>7.669797</td>\n      <td>-1.359096</td>\n      <td>-4.657529</td>\n      <td>...</td>\n      <td>0.000227</td>\n      <td>-0.000316</td>\n      <td>0.000413</td>\n      <td>0.000144</td>\n      <td>0.000721</td>\n      <td>-0.000532</td>\n      <td>0.000470</td>\n      <td>-0.000016</td>\n      <td>-0.000009</td>\n      <td>-3.934614e-06</td>\n    </tr>\n    <tr>\n      <th>3172</th>\n      <td>-4.387179</td>\n      <td>-4.060971</td>\n      <td>-7.321678</td>\n      <td>2.225588</td>\n      <td>2.264590</td>\n      <td>1.069938</td>\n      <td>0.334720</td>\n      <td>-3.195267</td>\n      <td>-2.207384</td>\n      <td>-5.335334</td>\n      <td>...</td>\n      <td>0.000872</td>\n      <td>-0.000283</td>\n      <td>-0.000864</td>\n      <td>-0.000395</td>\n      <td>-0.000498</td>\n      <td>-0.000170</td>\n      <td>0.000033</td>\n      <td>-0.000263</td>\n      <td>-0.000018</td>\n      <td>-3.317778e-06</td>\n    </tr>\n    <tr>\n      <th>3173</th>\n      <td>-4.627469</td>\n      <td>-6.619272</td>\n      <td>-8.359013</td>\n      <td>4.613657</td>\n      <td>0.688303</td>\n      <td>-0.457603</td>\n      <td>-1.968335</td>\n      <td>-3.471986</td>\n      <td>0.361107</td>\n      <td>-5.644895</td>\n      <td>...</td>\n      <td>-0.000321</td>\n      <td>-0.000440</td>\n      <td>0.001092</td>\n      <td>-0.000409</td>\n      <td>-0.000395</td>\n      <td>-0.000395</td>\n      <td>0.000123</td>\n      <td>0.000199</td>\n      <td>0.000023</td>\n      <td>-1.768249e-06</td>\n    </tr>\n    <tr>\n      <th>3174</th>\n      <td>-5.531907</td>\n      <td>-3.422538</td>\n      <td>-4.801727</td>\n      <td>-0.755215</td>\n      <td>-3.030000</td>\n      <td>-4.084527</td>\n      <td>0.745827</td>\n      <td>2.085324</td>\n      <td>-1.728323</td>\n      <td>-0.899399</td>\n      <td>...</td>\n      <td>0.000189</td>\n      <td>-0.000214</td>\n      <td>-0.000448</td>\n      <td>0.000365</td>\n      <td>0.000415</td>\n      <td>0.000190</td>\n      <td>-0.000121</td>\n      <td>-0.000129</td>\n      <td>-0.000018</td>\n      <td>-5.678798e-07</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1536 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pca = PCA()\n",
    "all_principal_components = all_pca.fit_transform(all_normalized_concat_features)\n",
    "all_principal_components_df = pd.DataFrame(all_principal_components)\n",
    "all_principal_components_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:39.328749822Z",
     "start_time": "2023-06-02T09:54:35.014674851Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity Check: Has the explained variance array the right shape, do the values add up to 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance array shape: (1536,)\n",
      "Sum of all explained variance values: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "all_explained_variance = all_pca.explained_variance_ratio_\n",
    "all_sum_of_explained_variance_values = np.sum(all_explained_variance)\n",
    "print(f\"Explained variance array shape: {all_explained_variance.shape}\")\n",
    "print(f\"Sum of all explained variance values: {all_sum_of_explained_variance_values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T10:06:20.375992501Z",
     "start_time": "2023-06-02T10:06:20.349230913Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have the explained variance for all the 1536 principal components. Let’s now sum over the first and last 768 principal components:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the first 768 PCs within all splits: 0.9348670840263367\n",
      "Explained variance of the last 768 PCs within all splits: 0.0651329830288887\n"
     ]
    }
   ],
   "source": [
    "all_txt_features_explained_variance = np.sum(all_explained_variance[:train_txt_emb.shape[1]])\n",
    "all_img_features_explained_variance = np.sum(all_explained_variance[-train_img_emb.shape[1]:])\n",
    "print(f\"Explained variance of the first 768 PCs within all splits: {all_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of the last 768 PCs within all splits: {all_img_features_explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:39.441586583Z",
     "start_time": "2023-06-02T09:54:39.354094667Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Summary of Results and Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results for train split and all data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of text embeddings within train split: 0.9474944472312927\n",
      "Explained variance of img embeddings within train split: 0.052505627274513245\n",
      "\n",
      "Explained variance of text embeddings within all splits: 0.9348670840263367\n",
      "Explained variance of img embeddings within all splits: 0.0651329830288887\n"
     ]
    }
   ],
   "source": [
    "print(f\"Explained variance of text embeddings within train split: {train_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of img embeddings within train split: {train_img_features_explained_variance}\\n\")\n",
    "\n",
    "print(f\"Explained variance of text embeddings within all splits: {all_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of img embeddings within all splits: {all_img_features_explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T09:54:43.349046100Z",
     "start_time": "2023-06-02T09:54:43.314818774Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The first 768 PCs capture around 95 % of the features' variance.\n",
    "- Even though the first PCs do not mathematically correspond to the first 768 features (i.e. the text embeddings), this fits our hypothesis.\n",
    "- Almost all the variance can be explained by half the number of dimensions - and half our dimensions are made up by image embedding dimensions.\n",
    "- This matches our previous findings: Training an SVM/VanillaNN on text only yields hardly worse results than training on the concat features."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cw1_kernel",
   "language": "python",
   "display_name": "cw1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
