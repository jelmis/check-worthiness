{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PCA\n",
    "In this notebook, we conduct a PCA on our concat features. We want to check, if the explained variance of the principal components fit our hypothesis that the image embeddings do not add any significant information.\n",
    "\n",
    "We will conduct a PCA for\n",
    "- the training split\n",
    "- all splits combined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports and Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-01T14:27:20.769415505Z",
     "start_time": "2023-06-01T14:27:20.423732186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# AUTORELOAD\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# GENERAL IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TASK-SPECIFIC IMPORTS\n",
    "from src import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CONSTANTS\n",
    "users = [\"patriziopalmisano\", \"onurdenizguler\", \"jockl\"]\n",
    "TRAIN = \"train\"\n",
    "DEV = \"dev\"\n",
    "TEST = \"test\"\n",
    "\n",
    "####################### SELECT ###########################\n",
    "user = users[2] # SELECT USER\n",
    "version = \"v2\" # SELECT DATASET VERSION\n",
    "dataset_version = version\n",
    "##########################################################\n",
    "\n",
    "if user in users[:2]:\n",
    "    data_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/data/CT23_1A_checkworthy_multimodal_english_{version}\"\n",
    "    cw_dir = f\"/Users/{user}/Library/CloudStorage/GoogleDrive-check.worthiness@gmail.com/My Drive/\"\n",
    "\n",
    "else:\n",
    "    data_dir = f\"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive/data/CT23_1A_checkworthy_multimodal_english_{dataset_version}\"\n",
    "    cw_dir = \"/home/jockl/Insync/check.worthiness@gmail.com/Google Drive\"\n",
    "\n",
    "features_dir = f\"{data_dir}/features\"\n",
    "labels_dir = f\"{data_dir}/labels\"\n",
    "models_dir = f\"{cw_dir}/models/vanillann\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Train Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Load Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first load all the features and compare their shapes and contents with the original embeddings. We want to make sure that the first 768 feature dimensions indeed belong to the text embeddings and the last 768 to the image embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "train_txt_emb, train_img_emb = utils.get_embeddings_from_pickle_file(f\"{data_dir}/embeddings_{TRAIN}_{dataset_version}.pickle\")\n",
    "train_concat_features = np.load(f\"{features_dir}/concat/concat_{TRAIN}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "print(f\"Train txt embeddings: {train_txt_emb.shape}\")\n",
    "print(f\"Train img embeddings: {train_img_emb.shape}\")\n",
    "print(f\"Train concat features: {train_concat_features.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spot check if the first 768 feature dimensions indeed belong to the text embeddings, the latter 768 to the image embeddings:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "print(f\"Train txt embd excerpt: {train_txt_emb[0][:5]}\")\n",
    "print(f\"Train features excerpt: {train_concat_features[0][:5]}\")\n",
    "print(f\"Train img embd excerpt: {train_img_emb[0][-5:]}\")\n",
    "print(f\"Train features excerpt: {train_concat_features[0][-5:]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Normalize the Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform a PCA, we first need to normalize the feature values. The normalized features should have a mean of 0, and standard deviation of 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "train_normalized_concat_features = StandardScaler().fit_transform(train_concat_features)\n",
    "print(f\"Train normalized concat features: {train_normalized_concat_features.shape}\")\n",
    "print(f\"Mean: {np.mean(train_normalized_concat_features)}\")\n",
    "print(f\"Standard Deviation: {np.std(train_normalized_concat_features)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean and standard deviation have the desired values, the features are now normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 PCA and Explained Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have normalized feature values, we can compute all principal components.\n",
    "\n",
    "IMPORTANT NOTE: There is no direct \"mapping\" between the n-th PC and the n-th feature dimension. The PCs are strictly ordered according to their explained variance values - by definition, the first PC explains the highest amount of variance, while this of course does not have to be the case for the first feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "train_pca = PCA()\n",
    "train_principal_components = train_pca.fit_transform(train_normalized_concat_features)\n",
    "train_principal_components_df = pd.DataFrame(train_principal_components)\n",
    "train_principal_components_df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity Check: Does the explained variance array have the right shape, do the values add up to 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "train_explained_variance = train_pca.explained_variance_ratio_\n",
    "sum_of_train_explained_variance_values = np.sum(train_explained_variance)\n",
    "print(f\"Explained variance per principal component array: {train_explained_variance.shape}\")\n",
    "print(f\"Sum of all explained variance values: {sum_of_train_explained_variance_values}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have the explained variance for all the 1536 principal components. Let's now sum over the first and last 768 values:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "train_txt_features_explained_variance = np.sum(train_explained_variance[:train_txt_emb.shape[1]])\n",
    "train_img_features_explained_variance = np.sum(train_explained_variance[-train_img_emb.shape[1]:])\n",
    "print(f\"Explained variance of the first 768 PCs within train split: {train_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of the last 768 PCs within train split: {train_img_features_explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. All splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Load Features\n",
    "Let's first load all the features and compare their shapes and contents with the original embeddings. We want to make sure that the first 768 feature dimensions indeed belong to the text embeddings and the last 768 to the image embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "dev_concat_features = np.load(f\"{features_dir}/concat/concat_{DEV}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "test_concat_features = np.load(f\"{features_dir}/concat/concat_{TEST}_{dataset_version}.pickle\", allow_pickle=True)\n",
    "all_concat_features = np.concatenate((train_concat_features, dev_concat_features, test_concat_features))\n",
    "print(f\"All concat features: {all_concat_features.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spot check if the first 768 feature dimensions indeed belong to the text embeddings, the latter 768 to the image embeddings:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "# Load test embeddings\n",
    "test_txt_emb, test_img_emb = utils.get_embeddings_from_pickle_file(f\"{data_dir}/embeddings_{TEST}_{dataset_version}.pickle\")\n",
    "\n",
    "# Spot check\n",
    "print(f\"Test txt embd excerpt: {test_txt_emb[-1][:5]}\")\n",
    "print(f\"Test features excerpt: {all_concat_features[-1][:5]}\")\n",
    "print(f\"Test img embd excerpt: {test_img_emb[-1][-5:]}\")\n",
    "print(f\"Test features excerpt: {all_concat_features[-1][-5:]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Normalize the Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform a PCA, we first need to normalize the feature values. The normalized features should have a mean of 0, and standard deviation of 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "all_normalized_concat_features = StandardScaler().fit_transform(all_concat_features)\n",
    "print(f\"Dev normalized concat features: {all_normalized_concat_features.shape}\")\n",
    "print(f\"Mean: {np.mean(all_normalized_concat_features)}\")\n",
    "print(f\"Standard Deviation: {np.std(all_normalized_concat_features)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean and standard deviation have the desired values, the features are now normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 PCA and Explained Variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have normalized feature values, we can compute all principal components.\n",
    "\n",
    "IMPORTANT NOTE: There is no direct \"mapping\" between the n-th PC and the n-th feature dimension. The PCs are strictly ordered according to their explained variance values - by definition, the first PC explains the highest amount of variance, while this of course does not have to be the case for the first feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "all_pca = PCA()\n",
    "all_principal_components = all_pca.fit_transform(all_normalized_concat_features)\n",
    "all_principal_components_df = pd.DataFrame(all_principal_components)\n",
    "all_principal_components_df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity Check: Has the explained variance array the right shape, do the values add up to 1?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "all_explained_variance = all_pca.explained_variance_ratio_\n",
    "all_sum_of_explained_variance_values = np.sum(all_explained_variance)\n",
    "print(f\"Explained variance per principal component array: {all_explained_variance.shape}\")\n",
    "print(f\"Sum of all explained variance values: {all_sum_of_explained_variance_values}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have the explained variance for all the 1536 principal components. Let’s now sum over the first and last 768 principal components:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "all_txt_features_explained_variance = np.sum(all_explained_variance[:train_txt_emb.shape[1]])\n",
    "all_img_features_explained_variance = np.sum(all_explained_variance[-train_img_emb.shape[1]:])\n",
    "print(f\"Explained variance of the first 768 PCs within all splits: {all_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of the last 768 PCs within all splits: {all_img_features_explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Summary of Results and Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results for train split and all data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "print(f\"Explained variance of text embeddings within train split: {train_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of img embeddings within train split: {train_img_features_explained_variance}\\n\")\n",
    "\n",
    "print(f\"Explained variance of text embeddings within all splits: {all_txt_features_explained_variance}\")\n",
    "print(f\"Explained variance of img embeddings within all splits: {all_img_features_explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The first 768 PCs capture around 95 % of the feature's variance.\n",
    "- Even though the first PCs do not mathematically translate to the first 768 features (i.e. the text embeddings), this fits our hypothesis.\n",
    "- Almost all the variance can be explained by half the number of dimensions - and half our dimensions are made up by image embedding dimensions.\n",
    "- This matches our previous findings: Training an SVM/VanillaNN on text only yields hardly worse results than training on the concat features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cw1_kernel",
   "language": "python",
   "display_name": "cw1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
